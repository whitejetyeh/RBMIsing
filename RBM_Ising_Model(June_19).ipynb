{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RBM_Ising_Model(June 19).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whitejetyeh/RBMIsing/blob/master/RBM_Ising_Model(June_19).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwFshnUEAZtP",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "From the physics point of view, Restrictive Boltzmann Machine (RBM), [Hinton and Salakhutdinov (07)](https://www.cs.toronto.edu/~hinton/science.pdf), extracts features of stochastic distributions in a coarse-graining process based on the concept of renormalization group (RG), see\n",
        "[Mehta and Schwab (14)](https://arxiv.org/abs/1410.3831). The standard RG procedure aims to extract the (scale-invariant) relevant features of a physical system at large scale by firstly marginalizing out the short distance degrees of freedom and secondly rescaling the system back with lower resolutions. Through iterative coarse-graining (marginalize and rescale), only relevant features are preserved in the renormalized system. Metha and Schwab (14) compared this coarse-graining procedure to how RBM encodes the information passing from the visual layer to the hidden layer in the context of the Ising model.\n",
        "\n",
        "Even though the exact map between RBM and RG depends on choosing the cost function in RBM concerning the relevant features in RG as criticized in [Lin et al. (17)](https://arxiv.org/abs/1608.08225), [Koch-Janusz and Ringel (18)](https://arxiv.org/abs/1704.06279) demonstrated that RBM is capable of extracting relevant features acquired in a general RG procedure. Here, we explicitly examed the information on visual and hidden layers to show that the hidden layer is not the coarse-grained visual layer as claimed in the exact map. \n",
        "\n",
        "The relevant features exist in the coarse-grained system in RG, but RBM keeps the features learned in weight and bias parameters, not the hidden layer.\n",
        "The proper analogy is to describe the reconstructed visual layer of RBM as the rescaled and coarse-grained system in RG. The hidden layer is merely an intermediate step of RBM with no counterpart in RG.\n",
        "\n",
        "To demonstrate how RBM works, we trained an RBM to reconstruct spin distributions and another classifier to read the temperature as the work done in [Iso et al. 18](https://arxiv.org/abs/1801.07172).\n",
        "(Note: the propose of relating the weight parameters to the recognition of the critical point was disproved recently in [Aoki et al. 19](https://arxiv.org/abs/1901.03817).)\n",
        "The spin distributions in the Ising model are stochastically generated binary values on a finite dimensional grid with periodic boundary conditions.\n",
        "\n",
        "\n",
        "Ising model exhibits many statistical properties observed similarly in economics, e.g., agents deciding to buy or sell in the stock market\n",
        "[Sornette & Zhou (05)](https://arxiv.org/abs/cond-mat/0503607),\n",
        "[Sornette (14)](https://arxiv.org/abs/1404.0243v1), and references therein.\n",
        "It is interesting to find that the characteristic length in a stochastic distribution can be easily learned by a neural network. \n",
        "Along the lines of an RBM reconstructs arbitrary inputs into distributions at the critical temperature, it is possible to construct an unsupervised learning to identify the phase transition of a stochastic distribution without any prior knowledge, where the transition distinguishes the herding phase of strong correlations and the chaotic phase with weak correlations.\n",
        "The application on Ising economics would be another interesting project in the future.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU241aCc6Q6a",
        "colab_type": "text"
      },
      "source": [
        "This project has three parts. The first two parts are in separate colab documents. The first part, [MCMC Ising Model](https://drive.google.com/open?id=1GTzxGebfb-vao0J4q_CzSwC3SPmWakAd), generates spin distributions from the 2D Ising model with Markov Chain Monte Carlo method. The second part, [Temp Classifier](https://drive.google.com/open?id=10qLExy89EIOC-xR3hcRo_ltklT51bdJC), classifies spin distributions into given temperature labels. The third part is this document RBM Ising Model. \n",
        "\n",
        "In the following section, we first briefly introduce the restricted Boltzmann machine and then build one with contrastive divergence method in python. We then train our RBM at a fixed cold temperature, where we find, as expected, the RBM reconstructs the given spin distributions after many iterations from learned cold temperature feature. Further, we reproduce the finding in [Iso et al. 18](https://arxiv.org/abs/1801.07172) that the RBM trained with full temperature spectrum learns the scale invariant feature at the critical temperature $T_C$, and the temp classifier specifies the reconstruction from an arbitrary spin distribution at $T_C$ after enough iterations.\n",
        "\n",
        "In the last section, we demonstrate with a full spectrum RBM, whether the smaller hidden layer encodes the same feature as the input visual layer in larger dimensions. While the feature of the reconstructed visual layer maintains the general  shape as input, it is deformed toward distributions at $T_C$ in agreement with previous findings. However, the overall distributions on the hidden layer are completely deformed, and temp classifier reads different temperatures randomly. \n",
        "\n",
        "Surprisingly, even with a far different distribution in the hidden layer, RBM is able to reconstruct the visual layer similar to the input. RBM does not simply encode a low-resolution version of input on the hidden layer. **By storing the characteristic feature of training data in the weight matrix and bias vectors, RBM molds any input with the shape of the training data through the process passing information back and forth between visual and hidden layers until the reconstructed visual state gains the characteristic feature of training data.** Hence, the full spectrum RBM reconstructs any input into the distributions at $T_C$ because the characteristic feature shared with distributions at all temperature appears most frequently at the critical temperature when the distribution is scale-invariant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYPkwKMAT9hC",
        "colab_type": "text"
      },
      "source": [
        "# Restricted Boltzmann Machine (RBM)\n",
        "\n",
        "\n",
        "RBM is an unsupervised machine learning model consisted of binary variables on two layers, a visible layer taking input data and another hidden layer generated from the input with respect to trained parameters of weight $W$ and bias $(b^{v}, b^{h})$.\n",
        "The model considers a spin distribution of variables $v$ and $h$ on visible and hidden layers respectively. The variable takes binary values 1/0 to indicate spin up/down. Through the Hamiltonian\n",
        "$$E(v,h)=-b^{(v)}_iv^i-b^{(h)}_jh^j-v^iW_{ij}h^j,$$\n",
        "a probability to observe a configuration $(v,h)$ is given by the Boltzmann distribution\n",
        "$$p(v,h)=\\frac{e^{-E(v,h)}}{Z},\\ \\textrm{where } Z=\\sum\\limits_{v,h}e^{-E(v,h)}.$$\n",
        "The Hamiltonian is restricted to intra-layer couplings (hence the name Restricted Boltzmann Machine), and the restriction brings the convenience of computing the conditional probability $p(h|v)$, which then generates the spin distribution $h$ on the hidden layer based on the input distribution $v$ via parameters of weight $W$ and bias $(b^{v},b^{h})$.\n",
        "\n",
        "The process of learning parameters of weight $W$ and bias $(b^{v},b^{h})$ via the method of **contrastive divergence** (CD-k) is as following.\n",
        "\n",
        "\n",
        "1.   $W$, $b^{v}$, and $b^{h}$ are initialized randomly, and then one generates $h$ from the input $v$ via $p(v|h)$. \n",
        "2.   **Gibbs sampling**: denoting $(v,h)$ as $(v^{(0)},h^{(0)})$, one can iteratively reproduce $v^{(k)}$  from $h^{(k-1)}$ with $p(v^{(k)}|h^{(k-1)})$ and $h^{(k)}$ from $v^{(k)}$ with $p(h^{(k)}|v^{(k)})$ by k Gibbs steps.\n",
        "3. to learn from $N$ samples, weight and biases are updated by adding \n",
        "$$\\begin{align*}\\Delta W_{ij}&=\\frac{1}{N}\\sum\\limits_{a\\in \\textrm{samples}}v_i^{(0)}h_j^{(0)}-\\frac{1}{N}\\sum\\limits_{a\\in \\textrm{samples}}v_i^{(k)}h_j^{(k)},\\\\\n",
        "\\Delta b^{v}_{i}&=\\frac{1}{N}\\sum\\limits_{a\\in \\textrm{samples}}v_i^{(0)}-\\frac{1}{N}\\sum\\limits_{a\\in \\textrm{samples}}v_i^{(k)},\\\\\n",
        "\\Delta b^{h}_{j}&=\\frac{1}{N}\\sum\\limits_{a\\in \\textrm{samples}}h_j^{(0)}-\\frac{1}{N}\\sum\\limits_{a\\in \\textrm{samples}}h_j^{(k)}.\\end{align*}$$\n",
        "4. repeat steps 2 and 3 with updated weight and biases.\n",
        "\n",
        "Until this point, one is ready to implement RBM. Furthermore, to form a **deep belief network**, one can stack many RBMs on top of each other by feeding the next level RBM's visible layer with the hidden layer of a lower level RBM. Following paragraphs are devoted for theoretical details of RBM.\n",
        "The recommended references of RBMs are [Hinton (10)](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf) and\n",
        "[Salakhutdinov et al. (07)](http://www.cs.utoronto.ca/~hinton/absps/netflixICML.pdf).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ANw5S2xYSM1",
        "colab_type": "text"
      },
      "source": [
        "## Spin distributions on Visible and Hidden layers\n",
        "To compute spin distributions, we don't directly write out the probability $p(v,h)$ because of the difficulty summing all possible states in the partition function $Z$. However, due to the absence of the intra-layer interactions, it is easy to derive the conditional probabilities of $p(h|v)$ and $p(v|h)$ explicitly.\n",
        "\n",
        "First, consider the condional probabilities as the products of probability of each spin, $p(h|v)=\\prod_i p(h_j=1|v)$ and $p(v|h)=\\prod_i p(v_i=1|h)$, and then it's straighforward to eliminate $Z$ in each conditional spin probability as\n",
        "$$P(h_j=1|v)=\\frac{\\sum\\limits_{\\{h|h_j=1\\}}p(v,h)}{\\sum\\limits_{h}p(v,h)}=\\mathrm{sigmod}\\left(b^{(h)}_j+v^iW_{ij} \\right)$$\n",
        "and\n",
        "$$P(v_i=1|h)=\\frac{\\sum\\limits_{\\{v|v_i=1\\}}p(v,h)}{\\sum\\limits_{v}p(v,h)}=\\mathrm{sigmod}\\left(b^{(v)}_i+W_{ij} h^j \\right),$$\n",
        "where $\\mathrm{sigmod}(x)=\\frac{1}{1+exp(-x)}$.\n",
        "\n",
        "Second, the spin distribution on a layer is decided from the input on another layer by the Monte Carlo method (throwing a dice). For example, with the input $v$ and a dice randomly choosing a value in $[0,1]$, $h_j=1$ if dice$>p(h_j=1|v)$, otherwise $h_j=0$.\n",
        "\n",
        "### Kullback-Leibler divergence (i.e. cross entropy)\n",
        "The principle to update parameters is to minimized the Kullback-Leibler divergence, \n",
        "$$KL(q||p)=\\sum\\limits_v q(v) \\log\\frac{q(v)}{p(v)},$$\n",
        "which measures the difference between the data probability distribution $p(v)=\\sum_h p(v,h)$ and the true probability distribution $q(v)$. With a large but finite number of samplings, $p(v)$ is closed to $q(v)$. Note, $KL(q||p)=-\\sum_vq(v)\\log p(v)\\textrm{(cross entropy)}-\\textrm{(system entropy=const.)}$, so minimizing KL divergence is the same as minimizing the cross entropy, which is much frequently used in the data science field.\n",
        "\n",
        "KL divergence (positive semifefinite) will be smaller by updating parameters with gradient descent, e.g. $W_{ij}=W_{ij}^{0}-\\epsilon\\frac{\\partial KL}{\\partial W_{ij}}$, where $\\epsilon$ is the learning rate as an extra parameter. Very beutifully, one finds\n",
        "$$\\begin{align*}-\\frac{\\partial KL}{\\partial W_{ij}}&=\\left\\langle v_i h_j\\right\\rangle_{\\textrm{data}}-\\left\\langle v_i h_j\\right\\rangle_{\\textrm{model}},\\\\\n",
        "-\\frac{\\partial KL}{\\partial b_{i}^{(v)}}&=\\left\\langle v_i\\right\\rangle_{\\textrm{data}}-\\left\\langle v_i\\right\\rangle_{\\textrm{model}},\\\\\n",
        "-\\frac{\\partial KL}{\\partial b_{j}^{(h)}}&=\\left\\langle h_j\\right\\rangle_{\\textrm{data}}-\\left\\langle h_j\\right\\rangle_{\\textrm{model}},\\end{align*}$$\n",
        "where expectations are defined by\n",
        "$$\\begin{align*}\\left\\langle x(v)\\right\\rangle_{\\textrm{data}}&=\\sum\\limits_v q(v) x(v),\\\\\n",
        "\\left\\langle y(v,h)\\right\\rangle_{\\textrm{model}}&=\\sum\\limits_{v,h} p(v,h)y(v,h).\\end{align*}$$\n",
        "### Gibbs sampling\n",
        "To minimize $KL(q||p)$, one faces the difficulty of computing $p(v,h)$ once again. Similar to the mean field approximation in statistical physics, the contrastive divergence method suggests that Gibbs sampling runs a Markov chain to find the k-steps stochastic reconstruction \n",
        "$$v^{(k)}_i \\approx p(v^{(k)}_i|h^{(k-1)})\\textrm{ and }h^{(k)}_j \\approx p(h^{(k)}_j|v^{(k)}),$$\n",
        "which distributes in the neighborhood of $v^{0}_i$ and $h^{0}_j$ with respect to $p(v,h)$\n",
        "and therefore $N$ stochastic reconstructions from the dataset helps us to approximate $$\\sum\\limits_{v,h} p(v,h)y(v,h)\\approx \\frac{1}{N}\\sum\\limits_{a \\in \\textrm{samples}}y(v^{(k)},h^{(k)}).$$\n",
        "\n",
        "Suprisingly, although $k$ needs to be big enough for the Markov chain to converge, $k=1$ often gives good results, see ref(Bengio & Delalleau, 2007)\n",
        "\n",
        "According to ref(Hinton 2010-003), it's important to sample binary values on the data-driven hidden layer $h^{(0)}$, i.e. to pick 1 or 0 with probability $p(h_j=1|v)$. The fact that a hidden variable can convey at most one bit creates the information bottleneck, which acts as a strong regularizer. On the other hand, it is better to directly use the real-valued probabilities $p(v^{(l)}_i|h^{(l-1)})$ and $p(h^{(l)}_i|v^{(l)})$ as the stochastic reconstruction $v^{(l)}$ and $h^{(l)}$ for $l\\geq 1$ to reduce sampling noise thus allowing faster learning.\n",
        "Accordingly the code here  applies real-valued probabilities on all layers in Gibbs sampling except two first layers $v^{(0)}$ and $h^{(0)}$, which take a binary value from (1, 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWpg3kObUF4v",
        "colab_type": "text"
      },
      "source": [
        "## Python code for RBM\n",
        "RBMcdk is a class defined for restrictive boltzmann machine with CD-k method. \n",
        "One builds a model X with V and H variables in visible and hidden layers respectively by \n",
        "\n",
        "```\n",
        "X = RBMcdk(V,H).\n",
        "```\n",
        "The input data D with N samples of V-dimensional arrays shall be shaped as a 2D-array of (dim_0=N, dim_1=V).\n",
        "With CD-k method, one then trains the model X in N epochs with learning rate L by\n",
        "\n",
        "```\n",
        "X.contrastive_divergence(D,iterations=N,k=k).\n",
        "```\n",
        "\n",
        "After X is trained, Nd samples in a dataset Dd (dim_0=Nd, dim_1=V) can be processed into reconstructed Dd_recon by kd Gibbs steps with X by\n",
        "\n",
        "```\n",
        "Dd_recon = gibbs_sampling(Dd, X, k=kd, return_state).\n",
        "```\n",
        "\n",
        "For *return_state = V_gibbs, H_gibbs, or H_data*,  *gibbs_sampling* returns the gibbs visual state $v^{(k)}$, the gibbs hidden state $h^{(k)}$, or the data driven hidden state $h^{(0)}$ respectively.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkP8ya5KHY14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from scipy.special import expit # sigmoid\n",
        "\n",
        "# Functions useful in RBM\n",
        "def cond_prob(states, weight, bias):\n",
        "        # conditional probability\n",
        "        Nsamples = len(states)\n",
        "        stacked_bias = np.stack([bias for _ in range(Nsamples)],axis=0)\n",
        "        return expit(np.matmul(states, weight) + stacked_bias)\n",
        "    \n",
        "def MC_selection(probabilities):\n",
        "        # stochastically sample binary values with given probabilities\n",
        "        # compare probabilities P with a random tensor T in the same shape; \n",
        "        # return 1 for P > T or 0 for P <= T; P must be in [0,1].\n",
        "        coins = np.random.uniform(0,1,np.shape(probabilities))\n",
        "        return np.ceil(probabilities-coins)\n",
        "\n",
        "# Building RBM\n",
        "class RBMcdk:\n",
        "    '''Restrctive Boltzmann Machine with CD_k method'''\n",
        "    def __init__(self, visible_dim, hidden_dim): \n",
        "      \n",
        "        #initialize weight and biases\n",
        "        np.random.seed(624) # fix the same random initial values\n",
        "        self.weight = np.random.normal(loc=0,scale=0.1,size=[visible_dim, hidden_dim])\n",
        "        self.visible_bias = np.random.uniform(low=0,high=1,size=[visible_dim])\n",
        "        self.hidden_bias = np.random.uniform(low=0,high=1,size=[hidden_dim])\n",
        "        \n",
        "    \n",
        "    def contrastive_divergence(self, data, iterations, k=1, learning_rate=0.1, text_out=False):\n",
        "        # data input = [v1,v2,...,vN] for N samples, v_dim = [visible_dim]\n",
        "        start = time.time() # timing training process\n",
        "        \n",
        "        Nsamples = len(data)\n",
        "        print(\"Number of samples = \",Nsamples,\" with data dimension = \",len(data[0]))        \n",
        "        \n",
        "        for i in range(iterations):\n",
        "                \n",
        "            # initialize data-driven hidden layer\n",
        "            # state dimensions = (Nsamples, layer dim)\n",
        "            self.hidden_state = MC_selection(cond_prob(data, \n",
        "                                                       self.weight, \n",
        "                                                       self.hidden_bias)) \n",
        "            \n",
        "            #Gibbs sampling iteration\n",
        "            #CD1\n",
        "            self.visible_gibbstate = cond_prob(self.hidden_state, \n",
        "                                               np.transpose(self.weight), \n",
        "                                               self.visible_bias)\n",
        "            self.hidden_gibbstate = cond_prob(self.visible_gibbstate, \n",
        "                                              self.weight, \n",
        "                                              self.hidden_bias)\n",
        "            #CDK>1\n",
        "            for j in range(k-1):\n",
        "                self.visible_gibbstate = cond_prob(self.hidden_gibbstate, \n",
        "                                                   np.transpose(self.weight), \n",
        "                                                   self.visible_bias)\n",
        "                self.hidden_gibbstate = cond_prob(self.visible_gibbstate, \n",
        "                                                  self.weight, \n",
        "                                                  self.hidden_bias)\n",
        "            \n",
        "            # update weight and biases Contrastive Divergence\n",
        "            \n",
        "            # <vh>_data\n",
        "            vh_data = np.matmul(np.transpose(data),self.hidden_state)/Nsamples\n",
        "            # <vh>_model\n",
        "            vh_model = np.matmul(np.transpose(self.visible_gibbstate),\n",
        "                                 self.hidden_gibbstate)/Nsamples\n",
        "            self.weight += learning_rate*(vh_data-vh_model) # weight update\n",
        "            \n",
        "            v_data = np.mean(data,axis=0) # <v>_data\n",
        "            v_model = np.mean(self.visible_gibbstate,axis=0) # <v>_model\n",
        "            self.visible_bias += learning_rate*(v_data-v_model) # v_bias update\n",
        "            \n",
        "            h_data = np.mean(self.hidden_state,axis=0) # <h>_data\n",
        "            h_model = np.mean(self.hidden_gibbstate,axis=0) # <h>_model        \n",
        "            self.hidden_bias += learning_rate*(h_data-h_model) # h_bias update\n",
        "          \n",
        "        end = time.time() # timing training process\n",
        "        print('training completed within ',end-start)\n",
        "        \n",
        "        # save trained weight and biases for big data\n",
        "        if text_out:\n",
        "            file_name = time.localtime(time.time()) # use mins+secs as the name\n",
        "            file_name = str(file_name.tm_min)+str(file_name.tm_sec)\n",
        "            path = '/content/gdrive/My Drive/IsingModel/'\n",
        "                        \n",
        "            file_path = path+file_name+'_weight.csv'\n",
        "            with open(file_path,'wb+') as f:\n",
        "                np.savetxt(f,self.weight,fmt='%g',delimiter=',')\n",
        "                \n",
        "            file_path = path+file_name+'_vbias.csv'\n",
        "            with open(file_path,'wb+') as f:\n",
        "                np.savetxt(f,self.visible_bias,fmt='%g',delimiter=',')\n",
        "                \n",
        "            file_path = path+file_name+'_hbias.csv'\n",
        "            with open(file_path,'wb+') as f:\n",
        "                np.savetxt(f,self.hidden_bias,fmt='%g',delimiter=',')\n",
        "            print('Print model to %s_weight/bias.csv for big data' % (file_name,))\n",
        "      \n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGLsFEV7BRuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gibbs_sampling(data, trained_rbm, k=1, return_state='V_gibbs'):    \n",
        "    \n",
        "    # data driven hidden layer\n",
        "    hidden_state = MC_selection(cond_prob(data, \n",
        "                                          trained_rbm.weight, \n",
        "                                          trained_rbm.hidden_bias)) \n",
        "            \n",
        "    #Gibbs sampling iteration for data reconstruction\n",
        "    #CD1\n",
        "    visible_gibbstate = cond_prob(hidden_state, \n",
        "                                  np.transpose(trained_rbm.weight), \n",
        "                                  trained_rbm.visible_bias)\n",
        "    hidden_gibbstate = cond_prob(visible_gibbstate, \n",
        "                                 trained_rbm.weight, \n",
        "                                 trained_rbm.hidden_bias)\n",
        "    #CDK>1\n",
        "    for j in range(k-1):        \n",
        "        visible_gibbstate = cond_prob(hidden_gibbstate, \n",
        "                                      np.transpose(trained_rbm.weight), \n",
        "                                      trained_rbm.visible_bias)\n",
        "        hidden_gibbstate = cond_prob(visible_gibbstate, \n",
        "                                     trained_rbm.weight, \n",
        "                                     trained_rbm.hidden_bias)\n",
        "    # reconstructed layers\n",
        "    visible_gibbstate = MC_selection(visible_gibbstate)\n",
        "    hidden_gibbstate = MC_selection(hidden_gibbstate)\n",
        "    \n",
        "    if return_state == 'V_gibbs':\n",
        "        return visible_gibbstate\n",
        "    elif return_state == 'H_gibbs':\n",
        "        return hidden_gibbstate\n",
        "    elif return_state == 'H_data':\n",
        "        return hidden_state\n",
        "    else:\n",
        "        print('calling invalid return_state')\n",
        "        return visible_gibbstate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1-tWNuB7z4s",
        "colab_type": "text"
      },
      "source": [
        "The following code demonstrates how to run RBMcdk with a short piece of code. When test data is a stack of the same vector, RBMcdk easily captures the repeated feature with few iterations, and the reconstruction replicates the input test data. Furthermore, given another different vector, its reconstruction after few Gibbs steps also replicate the original test data. **This indicates a strong feature learned in RBM will merge in any data after enough Gibbs steps**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7IJfXCUJPpe",
        "colab_type": "code",
        "outputId": "2ccfeaef-777c-4712-8ad2-62913ab17e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "'''This part is a small test for repeated data. \n",
        "   Expect to find the same visible_gibbstate as testdata \n",
        "   because rbm shall find clear feature in repeated data.'''\n",
        "\n",
        "testdata = np.array([[0,1,1,0,1], [0,1,1,0,1], [0,1,1,0,1], [0,1,1,0,1]]) # repeat the same data\n",
        "#testdata = np.array([[0,1,1,0,1], [0,1,0,0,1], [0,0,1,0,1], [0,0,1,0,1]]) # less similar data\n",
        "\n",
        "# set RBM by entering dimensions of visible, hidder layers\n",
        "testrbm = RBMcdk(5,5)\n",
        "\n",
        "# put data in RBM with CDk method\n",
        "testrbm.contrastive_divergence(testdata,iterations=50,k=2)\n",
        "\n",
        "print('\\n trained reconstructed visible states = \\n',testrbm.visible_gibbstate)\n",
        "stacked_data = np.stack(testdata,axis=0)\n",
        "reconstruction_error = np.sum((testrbm.visible_gibbstate-stacked_data)**2)\n",
        "print('\\n reconstruction error = ',reconstruction_error)\n",
        "\n",
        "# process validation data with trained RBM\n",
        "# the validation data flows into the training data because of the repeated feature\n",
        "validation_data = np.array([[1,1,0,1,1]])\n",
        "reconstruction_data = gibbs_sampling(validation_data, testrbm, k=5)\n",
        "print('\\n %s is reconstructed to %s' % (str(validation_data), \n",
        "                                        str(reconstruction_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples =  4  with data dimension =  5\n",
            "training completed within  0.010631799697875977\n",
            "\n",
            " trained reconstructed visible states = \n",
            " [[0.03986688 0.96451554 0.96617885 0.04179925 0.9677512 ]\n",
            " [0.03986688 0.96451554 0.96617885 0.04179925 0.9677512 ]\n",
            " [0.04114863 0.96335767 0.96523498 0.04332622 0.96670981]\n",
            " [0.03986688 0.96451554 0.96617885 0.04179925 0.9677512 ]]\n",
            "\n",
            " reconstruction error =  0.02756851475775112\n",
            "\n",
            " [[1 1 0 1 1]] is reconstructed to [[-0.  1.  1. -0.  1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAJw1t0cAE4y",
        "colab_type": "text"
      },
      "source": [
        "## RBM learning a cold Ising Model\n",
        "\n",
        "In this subsection, we train an RBM, *Cold_IsingRBM*, to learn the general features of spin distributions at a fixed temperature $T=0.27$, which is lower than the critical temperature $T_C=2.27$ on the 2D grid. At a cold temperature, spins tend to flip together and form big chunk of areas with the same direction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUKW4tPBcbKu",
        "colab_type": "text"
      },
      "source": [
        "Here, we mount a google drive with datasets of spin distributions in the Ising model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLCEHB_xcX-I",
        "colab_type": "code",
        "outputId": "fc1671c4-185c-4003-96c1-9b8caab9d0e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrSr73P5wjkA",
        "colab_type": "text"
      },
      "source": [
        "Load spin data at $T=0.27$ and shuffle them for training as the following\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2_qJpKtN_3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Read Ising Model spin distributions at given temperatures to train RBM\n",
        "'''\n",
        "# given temperature labels from data set\n",
        "temperatures=[0.27]\n",
        "\n",
        "# Load spin data from CSV file\n",
        "def acquire_spin_data(file, dataset):    \n",
        "    raw_data = np.loadtxt(file,delimiter=\",\")\n",
        "    raw_data = (raw_data+1)/2 # scale data from spin=(1,-1) to (1,0)\n",
        "    #raw_data=raw_data.reshape((2000,400)) # reshape to 2k data of 20^2 flatten grid\n",
        "    raw_data=raw_data.reshape((2000,100)) # reshape to 2k data of 10^2 flatten grid\n",
        "    # tensor_util.py in tf takes only plain python lists or tuples but not numpy arrays\n",
        "    raw_data=raw_data.astype(np.float32).tolist() \n",
        "    dataset.extend(raw_data)\n",
        "\n",
        "train_data, test_data = [], []\n",
        "train_labels, test_labels = [], []\n",
        "ind = 0 # counter for the labels\n",
        "for temp in temperatures:\n",
        "    train_file = '/content/gdrive/My Drive/IsingModel/training2k/t='+str(temp)+'.csv'\n",
        "    test_file = '/content/gdrive/My Drive/IsingModel/validation2k/t='+str(temp)+'.csv'\n",
        "    acquire_spin_data(train_file, train_data)\n",
        "    acquire_spin_data(test_file, test_data)\n",
        "    train_labels += [ind]*2000\n",
        "    test_labels += [ind]*2000\n",
        "    ind += 1\n",
        "\n",
        "# shuffle dataset and labels simutaneously\n",
        "# array indexing\n",
        "def unity_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "train_data,train_labels=unity_shuffled_copies(np.array(train_data),np.array(train_labels))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABLb4NvqxUEr",
        "colab_type": "text"
      },
      "source": [
        "Load the [temperature classifier](https://drive.google.com/open?id=10qLExy89EIOC-xR3hcRo_ltklT51bdJC), which is a trained neural network classifing spin distributions into different temperature labels. The classifier loaded here specifies the temperatures from three groups of spin distributions at $T=\\{0.27,2.27,4.27\\}$ with accuracy 99%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RunIyYTaHYI",
        "colab_type": "code",
        "outputId": "9859b4fe-45b1-46e4-e946-daacd3af31cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "source": [
        "'''\n",
        "Load temperature classifier, which is a pretrained neural network\n",
        "'''\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "classified_temp=[0.27, 2.27, 4.27]\n",
        "#classified_temp=[0.27, 1.27, 2.27, 3.27, 4.27]\n",
        "\n",
        "# load model and architecture from a single file\n",
        "save_path = '/content/gdrive/My Drive/IsingModel/'\n",
        "model_name = 'temp_classifier_100_256_512_3.h5'\n",
        "model = keras.models.load_model((save_path+model_name))\n",
        "\n",
        "model.summary() # print the architecture of the model\n",
        "\n",
        "# evaluate accuracy\n",
        "test_loss, test_acc = model.evaluate(np.array(test_data).reshape((2000,10,10)),[0]*2000)\n",
        "# needs to set up labels manually for different temperature input\n",
        "# 3 labels: [0] for 0.27, [1] for 2.27, [2] for 4.27\n",
        "# 5 labels: [0] for 0.27, [1] for 1.27, [2] for 2.27, [3] for 3,27, [4] for 4.27\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0616 20:20:27.622460 139943656740736 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0616 20:20:27.623988 139943656740736 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0616 20:20:27.627388 139943656740736 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 158,979\n",
            "Trainable params: 158,979\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "2000/2000 [==============================] - 1s 422us/sample - loss: 0.0173 - acc: 0.9955\n",
            "Test accuracy: 0.9955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppWUQ_FcAgoz",
        "colab_type": "text"
      },
      "source": [
        "Cold_IsingRBM has 100 visual variables taking spin distributions on a 10 by 10 grid and 64 hidden variables.\n",
        "Trained weight and biases will be saved in csv files if *text_out=True*.\n",
        "It takes about 2 minutes to compute 2,000 epochs with 2000 samples on a $10^2$ grid. With GPU acceleration, the time is shorten to 1.5 minutes (1.3 times faster)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fI3LzUFPhnq",
        "colab_type": "code",
        "outputId": "b0367bf4-c3e7-4a1f-91ac-ac8d983eda88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "'''\n",
        "train RBMcdk with Ising Model\n",
        "'''\n",
        "\n",
        "# set RBM by entering dimensions of visible, hidder layers\n",
        "Cold_IsingRBM = RBMcdk(100,64)\n",
        "\n",
        "# put data in RBM with CDk method\n",
        "Cold_IsingRBM.contrastive_divergence(train_data,\n",
        "                                     iterations=2000,\n",
        "                                     k=2,\n",
        "                                     text_out=0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples =  2000  with data dimension =  100\n",
            "training completed within  92.69297671318054\n",
            "Print model to 4516_weight/bias.csv for big data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D05XgYFl9Nnl",
        "colab_type": "text"
      },
      "source": [
        "Load trained model as *loaded_RBM*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkcovcgWkhO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Load Trained Model from csv files'''\n",
        "loaded_RBM = RBMcdk(100,64) # need to know the dimensions of the loaded model\n",
        "\n",
        "file_name = str(2538) #saved model's time stamp, found in message above\n",
        "path = '/content/gdrive/My Drive/IsingModel/'\n",
        "                       \n",
        "file_path = path+file_name+'_weight.csv'    \n",
        "loaded_RBM.weight = np.loadtxt(file_path,delimiter=\",\")\n",
        "\n",
        "file_path = path+file_name+'_vbias.csv'    \n",
        "loaded_RBM.visual_bias = np.loadtxt(file_path,delimiter=\",\")                       \n",
        "\n",
        "file_path = path+file_name+'_hbias.csv'    \n",
        "loaded_RBM.hidden_bias = np.loadtxt(file_path,delimiter=\",\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zFkGFoo9a1w",
        "colab_type": "text"
      },
      "source": [
        "Plot reconstructed visual states after 2000 epochs and compare with the input, which is independent from the training_data, at the same temperature. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOKuddKnmfvN",
        "colab_type": "code",
        "outputId": "48c82c8b-54de-492f-cf37-e5ba4384780b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "# plot 8 spin distributions from the validation dataset\n",
        "np.random.seed(1)\n",
        "validation_data = [0]*8\n",
        "ind=np.random.randint(0,1999)\n",
        "plt.figure(figsize=(11,11))\n",
        "for i in range(8):\n",
        "    validation_data[i] = test_data[ind+i] #pick random 8 data\n",
        "    plt.subplot(1,8,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.reshape(validation_data[i],(10,10)), cmap=plt.cm.binary)\n",
        "    plt.xlabel(temperatures[test_labels[ind+i]])\n",
        "plt.show()\n",
        "\n",
        "# RECONSTRUCTION of validation test with trained RBM\n",
        "# plot 8 spin distributions from the reconstructed validation dataset\n",
        "\n",
        "# process validation data with trained RBM\n",
        "reconstructed_data = gibbs_sampling(validation_data, loaded_RBM, k=2000)\n",
        "#reconstructed_data = gibbs_sampling(validation_data, Cold_IsingRBM, k=2000)\n",
        "reconstructed_label = [0]*8\n",
        "plt.figure(figsize=(11,11))\n",
        "for i in range(8):\n",
        "    \n",
        "    # identify temperature with Temp Classifier\n",
        "    img = np.reshape(np.array(reconstructed_data[i]),(10,10))\n",
        "    img = (np.expand_dims(img,0)) # Add the image to a batch where it's the only member.\n",
        "    predictions = model.predict(img)\n",
        "    reconstructed_label[i] = np.argmax(predictions)\n",
        "    \n",
        "    plt.subplot(1,8,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.reshape(reconstructed_data[i],(10,10)), cmap=plt.cm.binary)\n",
        "    plt.xlabel(classified_temp[reconstructed_label[i]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAABhCAYAAACu5mdDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABHlJREFUeJzt3b9r3HUcx/HXpwYHR0kHsdJTcFEc\nJMG1BBzEQR1EHNycXJ2Ebv4z6iS4FEFx7ZCD6iAotaSogxgcxR/DxyEnniGFfD/N5e7ePB7wheTy\nveN7r/aSJ01KWu89AADUdGXdFwAAwOqIPQCAwsQeAEBhYg8AoDCxBwBQmNgDAChM7AEAFCb2AAAK\nE3sAAIWJPQCAwnamnLy7u9tns9mKLmU7HB0d5fj4uE25j90ub7f5fD7p/FXY29u70Mebz+fHvfer\nU+7j75zX6qiR3Vprk3/v5kW/Tkas4POF1+oAr9Vx5/36MCn2ZrNZDg8Px6+qgP39/cn3sdvl7dba\npM8XK3HRf9attftT7+PvnNfqqJHdRmzCziv4fOG1OsBrddx5vz74Ni4AQGFiDwCgMLEHAFCY2AMA\nKEzsAQAUJvYAAAoTewAAhYk9AIDCxB4AQGFiDwCgMLEHAFCY2AMAKEzsAQAUJvYAAAoTewAAhYk9\nAIDCxB4AQGFiDwCgMLEHAFCY2AMAKEzsAQAUJvYAAAoTewAAhYk9AIDCxB4AQGFiDwCgMLEHAFCY\n2AMAKEzsAQAUJvYAAAoTewAAhYk9AIDCxB4AQGFiDwCgMLEHAFCY2AMAKEzsAQAUJvYAAAoTewAA\nhYk9AIDCxB4AQGFiDwCgMLEHAFCY2AMAKEzsAQAUJvYAAAoTewAAhYk9AIDCxB4AQGFiDwCgMLEH\nAFCY2AMAKEzsAQAUJvYAAAoTewAAhYk9AIDCxB4AQGFiDwCgMLEHAFCY2AMAKEzsAQAUJvYAAAoT\newAAhbXe+/lPbu3XJPdXdzlb4Xrv/eqUO9gtid0ehu3G2G2M3cbZbozdxp1ru0mxBwDAdvFtXACA\nwsQeAEBhGxN7rbVXWmvftdbuttY+OOPj77fWvm2tfdNa+7K1dn1x+0Fr7c7S8Udr7Y3LfwbrYbdx\nthtjtzF2G2e7MXYbU3K33vvajySPJPkhyTNJHk3ydZLnTp1zkOSxxdvvJfnkjMd5PMlv/55X/bCb\n7ey2HYfdbGe37Tiq7rYp/7L3UpK7vfd7vfe/knyc5PXlE3rvX/Xef1+8ezvJtTMe580kt5bOq85u\n42w3xm5j7DbOdmPsNqbkbpsSe08m+XHp/Z8Wtz3Iu0lunXH720k+usDr2nR2G2e7MXYbY7dxthtj\ntzEld9tZ9wVM1Vp7J8l+khunbn8iyQtJPl/HdW06u42z3Ri7jbHbONuNsduYbdptU2Lv5yRPLb1/\nbXHb/7TWXk5yM8mN3vufpz78VpJPe+9/r+wqN4/dxtlujN3G2G2c7cbYbUzN3db9Q4P95AcZd5Lc\nS/J0/vuByOdPnfNiTn5o8tkHPMbtJAfrfi52247Ddnaz23YctrOb3S7gea37ApbGeTXJ94sBby5u\n+zDJa4u3v0jyS5I7i+OzpfvOclLeV9b9POy2PYft7Ga37ThsZze7Pdzh16UBABS2Kf8bFwCAFRB7\nAACFiT0AgMLEHgBAYWIPAKAwsQcAUJjYAwAoTOwBABT2D3twYKQ1b+NKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 792x792 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAABhCAYAAACu5mdDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGjsv9mh1BLpSUG+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 792x792 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKMpm8lL9k7C",
        "colab_type": "text"
      },
      "source": [
        "The similar input and reconstructed data indicate the Cold_IsingRBM can maintain the feature learned at a fixed temperature as expected. The reconstruction differs with few pixels due to the stochastic process. Notice that the system does not distinguish the all spin up distribution from the all spin down one, so all white plot is considered the same as all black plot. With few pixels flipped, temp classifier reads higher temperatures from the almost all white/black plot especially due to the very short characteristic length from few pixels. This problem can be solved with higher resolutions or simply by discarding all white/black data at low temperature. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoiIsDg3BIph",
        "colab_type": "text"
      },
      "source": [
        "## RBM learning Ising Model from cold to hot\n",
        "\n",
        "With data from the full spectrum, we train the RBM to find how the general feature shared from low to high temperature adopts arbitrary input into the distributions at the critical temperature. \n",
        "\n",
        "We demonstrate the training with the data on a 10 by 10 grid with three temperature labels $\\{0.27,2.27,4.27\\}$. Waiting for the longer computation time, one can change to the data on a 20 by 20 grid for a higher resolution. However, due to the stochastic nature on the finite dimensional grid, the accuracy of temp classifier is significantly lower with five or more temperature labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErYpUHOorMBw",
        "colab_type": "code",
        "outputId": "24ceb19c-ec75-415d-863f-ba9750741955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "'''\n",
        "Read Ising Model spin distributions at given temperatures to train RBM\n",
        "'''\n",
        "# given temperature labels from data set\n",
        "#temperatures=[0.27, 1.27, 2.27, 3.27, 4.27]\n",
        "temperatures=[0.27, 2.27, 4.27]\n",
        "\n",
        "# Load spin data from CSV file\n",
        "def acquire_spin_data(file, dataset):    \n",
        "    raw_data = np.loadtxt(file,delimiter=\",\")\n",
        "    raw_data = (raw_data+1)/2 # scale data from spin=(1,-1) to (1,0)\n",
        "    #raw_data=raw_data.reshape((2000,400)) # reshape to 2k data of 20^2 flatten grid\n",
        "    raw_data=raw_data.reshape((2000,100)) # reshape to 2k data of 10^2 flatten grid\n",
        "    # tensor_util.py in tf takes only plain python lists or tuples but not numpy arrays\n",
        "    raw_data=raw_data.astype(np.float32).tolist() \n",
        "    dataset.extend(raw_data)\n",
        "\n",
        "train_data, test_data = [], []\n",
        "train_labels, test_labels = [], []\n",
        "ind = 0 # counter for the labels\n",
        "for temp in temperatures:\n",
        "    train_file = '/content/gdrive/My Drive/IsingModel/training2k/t='+str(temp)+'.csv'\n",
        "    test_file = '/content/gdrive/My Drive/IsingModel/validation2k/t='+str(temp)+'.csv'\n",
        "    acquire_spin_data(train_file, train_data)\n",
        "    acquire_spin_data(test_file, test_data)\n",
        "    train_labels += [ind]*2000\n",
        "    test_labels += [ind]*2000\n",
        "    ind += 1\n",
        "\n",
        "# shuffle dataset and labels simutaneously\n",
        "# array indexing\n",
        "def unity_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "train_data,train_labels=unity_shuffled_copies(np.array(train_data),np.array(train_labels))\n",
        "\n",
        "'''\n",
        "Load temperature classifier, which is a pretrained neural network\n",
        "'''\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "classified_temp=[0.27, 2.27, 4.27]\n",
        "#classified_temp=[0.27, 1.27, 2.27, 3.27, 4.27]\n",
        "\n",
        "# load model and architecture from a single file\n",
        "save_path = '/content/gdrive/My Drive/IsingModel/'\n",
        "model_name = 'temp_classifier_100_256_512_3.h5'\n",
        "model = keras.models.load_model((save_path+model_name))\n",
        "\n",
        "model.summary() # print the architecture of the model\n",
        "\n",
        "# evaluate accuracy\n",
        "test_loss, test_acc = model.evaluate(np.array(test_data).reshape((2000*3,10,10)),\n",
        "                                     [0]*2000+[1]*2000+[2]*2000)\n",
        "# needs to set up labels manually for different temperature input\n",
        "# 3 labels: [0] for 0.27, [1] for 2.27, [2] for 4.27\n",
        "# 5 labels: [0] for 0.27, [1] for 1.27, [2] for 2.27, [3] for 3,27, [4] for 4.27\n",
        "print('Test accuracy:', test_acc)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 158,979\n",
            "Trainable params: 158,979\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "6000/6000 [==============================] - 0s 44us/sample - loss: 0.1006 - acc: 0.9657\n",
            "Test accuracy: 0.96566665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBdfm71R--Xc",
        "colab_type": "text"
      },
      "source": [
        "Full_IsingRBM takes about 6 minutes to compute 2,000 epochs with 6,000 samples on a 10^2 grid. With GPU acceleration, the time is shorten to 5 minutes (1.2 times faster)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXx6pbMiEID4",
        "colab_type": "code",
        "outputId": "fc42a43e-5ee1-41b2-a064-84a1d56aef04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "'''\n",
        "train RBMcdk with Ising Model\n",
        "'''\n",
        "\n",
        "# set RBM by entering dimensions of visible, hidder layers\n",
        "Full_IsingRBM = RBMcdk(100,64) # full spectrum\n",
        "\n",
        "# put data in RBM with CDk method\n",
        "Full_IsingRBM.contrastive_divergence(train_data,\n",
        "                                     iterations=2000,\n",
        "                                     k=2,\n",
        "                                     text_out=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples =  6000  with data dimension =  100\n",
            "training completed within  295.2187592983246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCIKc2qWfTWs",
        "colab_type": "text"
      },
      "source": [
        "The following code uses the Full_IsingRBM trained with 10,000 epochs to reconstruct input with 2,000 iterations. The input includes random 8 spin distributions at $T_H=4.27$ and $T_L=0.27$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCLwUT1GsdsV",
        "colab_type": "code",
        "outputId": "85419d49-a752-4330-e1d7-74ad1ab7d514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "'''Load Trained Model from csv files'''\n",
        "loaded_RBM = RBMcdk(100,64) # need to know the dimensions of the loaded model\n",
        "\n",
        "# saved model's time stamp, found in message above\n",
        "#file_name = str(4546) # 4546 computes 10^2 grid for 2000 epochs.\n",
        "file_name = str(5118) # 5118 computes 10^2 grid for 10000 epochs.\n",
        "path = '/content/gdrive/My Drive/IsingModel/'\n",
        "                       \n",
        "file_path = path+file_name+'_weight.csv'    \n",
        "loaded_RBM.weight = np.loadtxt(file_path,delimiter=\",\")\n",
        "\n",
        "file_path = path+file_name+'_vbias.csv'    \n",
        "loaded_RBM.visual_bias = np.loadtxt(file_path,delimiter=\",\")                       \n",
        "\n",
        "file_path = path+file_name+'_hbias.csv'    \n",
        "loaded_RBM.hidden_bias = np.loadtxt(file_path,delimiter=\",\")\n",
        "\n",
        "# plot 8 spin distributions from the validation dataset\n",
        "'''\n",
        "Full_IsingRBM transforms High T distributions to Critical Tc distribution.\n",
        "'''\n",
        "tempind = 2  # pick a temp catagory, (0,1,2) = (T_L,T_C,T_H)\n",
        "validation_data = [0]*8\n",
        "np.random.seed(123)\n",
        "\n",
        "ind = 2000*tempind+np.random.randint(0,1999)\n",
        "print('Graphs below show %s distributions at %s%g.' % ('sample','T_H = ',temperatures[tempind]))\n",
        "plt.figure(figsize=(11,11))\n",
        "for i in range(8):\n",
        "    validation_data[i] = test_data[ind+i] #pick random 8 data\n",
        "    plt.subplot(1,8,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.reshape(validation_data[i],(10,10)), cmap=plt.cm.binary)\n",
        "    plt.xlabel(temperatures[test_labels[ind+i]])\n",
        "plt.show()\n",
        "\n",
        "# RECONSTRUCTION of validation test with trained RBM\n",
        "# plot 8 spin distributions from the reconstructed validation dataset\n",
        "\n",
        "# process validation data with trained RBM\n",
        "reconstructed_data = gibbs_sampling(validation_data, loaded_RBM, k=2000)\n",
        "#reconstructed_data = gibbs_sampling(validation_data, Full_IsingRBM, k=2000)\n",
        "reconstructed_label = [0]*8\n",
        "print('Graphs below show %s distributions.' % ('reconstructed',))\n",
        "plt.figure(figsize=(11,11))\n",
        "for i in range(8):\n",
        "    \n",
        "    # identify temperature with Temp Classifier\n",
        "    img = np.reshape(np.array(reconstructed_data[i]),(10,10))\n",
        "    img = (np.expand_dims(img,0)) # Add the image to a batch where it's the only member.\n",
        "    predictions = model.predict(img)\n",
        "    reconstructed_label[i] = np.argmax(predictions)\n",
        "    \n",
        "    plt.subplot(1,8,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.reshape(reconstructed_data[i],(10,10)), cmap=plt.cm.binary)\n",
        "    plt.xlabel(classified_temp[reconstructed_label[i]])\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "Full_IsingRBM transforms Low T distributions to Critical Tc distribution.\n",
        "'''\n",
        "tempind = 0  # pick a temp catagory, (0,1,2) = (T_L,T_C,T_H)\n",
        "validation_data = [0]*8\n",
        "np.random.seed(12)\n",
        "\n",
        "ind = 2000*tempind+np.random.randint(0,1999)\n",
        "print('Graphs below show %s distributions at %s%g.' % ('sample','T_L = ',temperatures[tempind]))\n",
        "plt.figure(figsize=(11,11))\n",
        "for i in range(8):\n",
        "    validation_data[i] = test_data[ind+i] #pick random 8 data\n",
        "    plt.subplot(1,8,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.reshape(validation_data[i],(10,10)), cmap=plt.cm.binary)\n",
        "    plt.xlabel(temperatures[test_labels[ind+i]])\n",
        "plt.show()\n",
        "\n",
        "# RECONSTRUCTION of validation test with trained RBM\n",
        "# plot 8 spin distributions from the reconstructed validation dataset\n",
        "\n",
        "# process validation data with trained RBM\n",
        "reconstructed_data = gibbs_sampling(validation_data, loaded_RBM, k=2000)\n",
        "#reconstructed_data = gibbs_sampling(validation_data, Full_IsingRBM, k=2000)\n",
        "reconstructed_label = [0]*8\n",
        "print('Graphs below show %s distributions.' % ('reconstructed',))\n",
        "plt.figure(figsize=(11,11))\n",
        "for i in range(8):\n",
        "    \n",
        "    # identify temperature with Temp Classifier\n",
        "    img = np.reshape(np.array(reconstructed_data[i]),(10,10))\n",
        "    img = (np.expand_dims(img,0)) # Add the image to a batch where it's the only member.\n",
        "    predictions = model.predict(img)\n",
        "    reconstructed_label[i] = np.argmax(predictions)\n",
        "    \n",
        "    plt.subplot(1,8,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.reshape(reconstructed_data[i],(10,10)), cmap=plt.cm.binary)\n",
        "    plt.xlabel(classified_temp[reconstructed_label[i]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graphs below show sample distributions at T_H = 4.27.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAABhCAYAAACu5mdDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 792x792 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Graphs below show reconstructed distributions.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAABhCAYAAACu5mdDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAICOKfYAADqm2AMA6Nj/AEJuHUoNJOVGAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 792x792 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Graphs below show sample distributions at T_L = 0.27.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAABhCAYAAACu5mdDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABKhJREFUeJzt3b+LXFUYBuD3xMXCUpJCjGQUbBQL\n2cU2BCzEQi1ELOysbK2EdP4zaiXYBEGxTZGBaCEoMSSohbhYij+KY7EjjsvusnMymzvz7fPAgZk7\ndy5nXs5cXmbuMK33HgAAarow9QQAADg7yh4AQGHKHgBAYcoeAEBhyh4AQGHKHgBAYcoeAEBhyh4A\nQGHKHgBAYcoeAEBhO6vsfPHixT6bzc5oKtvh3r172d/fb6s8p7V27H/S7e7uPvikzsh8Pl/r8Xrv\nK+VmvR2Yz+f7vfdLqzznpDV3nlhzqxs5x52U27rPIyMe1nl25L1qza1/zZ0np11zK5W92WyWW7du\njc+qgL29vbUeb5PzbG2l997aWW8HWmv3p57DeWHNjZ3jTspt6vNI8vDOsyPvVWtu/WvuPDntmvM1\nLgBAYcoeAEBhyh4AQGHKHgBAYSv9QAPYDru7u+f+4uV1/5gKYFv5ZA8AoDBlDwCgMGUPAKAwZQ8A\noDBlDwCgMGUPAKAwZQ8AoDBlDwCgMGUPAKAwZQ8AoDBlDwCgMGUPAKAwZQ8AoDBlDwCgMGUPAKAw\nZQ8AoDBlDwCgMGUPAKAwZQ8AoDBlDwCgMGUPAKAwZQ8AoDBlDwCgMGUPAKAwZQ8AoDBlDwCgMGUP\nAKAwZQ8AoDBlDwCgMGUPAKAwZQ8AoDBlDwCgsJ2pJwCs33w+T2tt6mkAsAF8sgcAUJiyBwBQmLIH\nAFCYsgcAUJiyBwBQmLIHAFCYsgcAUJiyBwBQmLIHAFCYsgcAUJiyBwBQmLIHAFCYsgcAUJiyBwBQ\nmLIHAFCYsgcAUJiyBwBQmLIHAFCYsgcAUJiyBwBQmLIHAFCYsgcAUJiyBwBQmLIHAFCYsgcAUFjr\nvZ9+59Z+TXL/7KazFa703i+t8gS5JZHbg5DdGLmNkds42Y2R27hTZbdS2QMAYLv4GhcAoDBlDwCg\nsI0pe621V1pr37XW7rTWPjji8fdba9+21r5prX3ZWruy2H6ttXZ7afzRWnvj4b+CachtnOzGyG2M\n3MbJbozcxpTMrfc++UjySJIfkjyT5NEkXyd57tA+15I8trj9XpJPjjjO40l++3e/6kNuspPbdgy5\nyU5u2zGq5rYpn+y9lORO7/1u7/2vJB8neX15h977V7333xd3bya5fMRx3kxyY2m/6uQ2TnZj5DZG\nbuNkN0ZuY0rmtill78kkPy7d/2mx7TjvJrlxxPa3k3y0xnltOrmNk90YuY2R2zjZjZHbmJK57Uw9\ngVW11t5Jspfk6qHtTyR5IcnnU8xr08ltnOzGyG2M3MbJbozcxmxTbptS9n5O8tTS/cuLbf/TWns5\nyfUkV3vvfx56+K0kn/be/z6zWW4euY2T3Ri5jZHbONmNkduYmrlNfdFgP7iQcSfJ3SRP578LIp8/\ntM+LObho8tljjnEzybWpX4vctmPITm5y244hO7nJbQ2va+oJLIXzapLvFwFeX2z7MMlri9tfJPkl\nye3F+GzpubMcNO8LU78OuW3PkJ3c5LYdQ3Zyk9uDDX+XBgBQ2Kb8GhcAgDOg7AEAFKbsAQAUpuwB\nABSm7AEAFKbsAQAUpuwBABSm7AEAFPYPpqNfso9ZhyYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 792x792 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Graphs below show reconstructed distributions.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAABhCAYAAACu5mdDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 792x792 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuI8baOvkSad",
        "colab_type": "text"
      },
      "source": [
        "In the graphs above, the first two rows and the last two rows compare the input and corresponding reconstructions after 2,000 iterations of eight distributions at $T_H=4.27$ and $T_L=0.27$. \n",
        "Playing with iterations k, (the number of Gibbs steps), one can see how distributions are gradually deformed into the ones at $T_C$. For distributions at $T_H$, the larger chunk of the white/black areas grow even bigger and reflect the shrinking characteristic length toward lower temperature, and distributions at $T_L$ undergo the opposite deformation toward higher temperature.\n",
        "One can easily compute many Gibbs steps over $k=10,000$ with *gibbs_sampling*. Once the temperature reading turns $T=T_C$, it stays the same while the distribution fluctuates with more Gibbs steps. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9SG42VH1eDJ",
        "colab_type": "text"
      },
      "source": [
        "# Information encoded on the hidden layer\n",
        "With the RBM learned critical features of spin distributions at $T_C$, we exam the information encoded on the hidden layer to find if the hidden state appears as the coarse-grained visual state as claimed in the exact map, [Mehta and Schwab (14)](https://arxiv.org/abs/1410.3831). \n",
        "The RBM used here has 144 visual dimensions for spins on a 14 by 14 grid and 100 hidden dimensions for spins on a 10 by 10 grid, and it is trained with full spectrum data to learn the distributions at $T_C$. We then reconstruct several spin distributions at $T_C$ and compare the reconstructed visual and hidden states with the input. While the reconstructed visual states preserve most significant chunks of areas, the data-driven and reconstructed hidden layers do not assemble the input, and the corresponding temperature readings vary randomly. Therefore, we do not view the hidden states as the coarse-grained visual states.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUP_ajgBjG_W",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load datasets and classifiers\n",
        "\n",
        "Prepare **train_data** and **test_data** from spin distributions on a 12^2 grid. There are three temperature labels, T_H=4.27, T_C=2.27, T_D=0.27.\n",
        "Two temp classifiers with 97+% accuracy are loaded. temp_classifier_144_256_512_3 and temp_classifier_100_256_512_3 specify temperature labels from 12^2 and 10^2 grids respectively, and they both have two hidden layers of 256 and 512 neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATesQdPBWBqP",
        "colab_type": "code",
        "outputId": "3359b1a3-34f4-4dab-b817-8f7991b0d08b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        }
      },
      "source": [
        "'''\n",
        "Read Ising Model spin distributions at given temperatures to train RBM\n",
        "'''\n",
        "# given temperature labels from data set\n",
        "#temperatures=[0.27, 1.27, 2.27, 3.27, 4.27]\n",
        "temperatures=[0.27, 2.27, 4.27]\n",
        "\n",
        "# Load spin data from CSV file\n",
        "def acquire_spin_data(file, dataset):    \n",
        "    raw_data = np.loadtxt(file,delimiter=\",\")\n",
        "    raw_data = (raw_data+1)/2 # scale data from spin=(1,-1) to (1,0)\n",
        "    raw_data=raw_data.reshape((2000,144)) # reshape to 2k data of 12^2 flatten grid    \n",
        "    # tensor_util.py in tf takes only plain python lists or tuples but not numpy arrays\n",
        "    raw_data=raw_data.astype(np.float32).tolist() \n",
        "    dataset.extend(raw_data)\n",
        "\n",
        "train_data, test_data = [], []\n",
        "train_labels, test_labels = [], []\n",
        "ind = 0 # counter for the labels\n",
        "for temp in temperatures:\n",
        "    train_file = '/content/gdrive/My Drive/IsingModel/training2kd12/t='+str(temp)+'.csv'\n",
        "    test_file = '/content/gdrive/My Drive/IsingModel/validation2kd12/t='+str(temp)+'.csv'\n",
        "    acquire_spin_data(train_file, train_data)\n",
        "    acquire_spin_data(test_file, test_data)\n",
        "    train_labels += [ind]*2000\n",
        "    test_labels += [ind]*2000\n",
        "    ind += 1\n",
        "\n",
        "# shuffle dataset and labels simutaneously\n",
        "# array indexing\n",
        "def unity_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "train_data,train_labels=unity_shuffled_copies(np.array(train_data),np.array(train_labels))\n",
        "\n",
        "'''\n",
        "Load temperature classifier, which is a pretrained neural network\n",
        "'''\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "classified_temp=[0.27, 2.27, 4.27]\n",
        "#classified_temp=[0.27, 1.27, 2.27, 3.27, 4.27]\n",
        "\n",
        "# load model and architecture from a single file\n",
        "save_path = '/content/gdrive/My Drive/IsingModel/'\n",
        "modeld12_name = 'temp_classifier_144_256_512_3.h5'\n",
        "modeld12 = keras.models.load_model((save_path+modeld12_name))\n",
        "\n",
        "modeld12.summary() # print the architecture of the model\n",
        "\n",
        "# evaluate accuracy\n",
        "test_loss, test_acc = modeld12.evaluate(np.array(test_data).reshape((2000*3,12,12)),\n",
        "                                     [0]*2000+[1]*2000+[2]*2000)\n",
        "# needs to set up labels manually for different temperature input\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "modeld10_name = 'temp_classifier_100_256_512_3.h5'\n",
        "modeld10 = keras.models.load_model((save_path+modeld10_name))\n",
        "\n",
        "modeld10.summary() # print the architecture of the model\n",
        "\n",
        "'''\n",
        "#modeld10 has accuracy~0.97, checked above.\n",
        "#accuracy is not evaluated again because test_data is on 12^2 grid instead of 10^2 grid.\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 144)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               37120     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 170,243\n",
            "Trainable params: 170,243\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "6000/6000 [==============================] - 0s 47us/sample - loss: 0.0617 - acc: 0.9802\n",
            "Test accuracy: 0.9801667\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 158,979\n",
            "Trainable params: 158,979\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#modeld10 has accuracy~0.97, checked above.\\n#accuracy is not evaluated again because test_data is on 12^2 grid instead of 10^2 grid.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUErS5vH0zrl",
        "colab_type": "text"
      },
      "source": [
        "## 2. Train the model\n",
        "Train an RBM (CD-k=2) called Full_encoder, which has a 12^2 dimensional visual layer and a 10^2 dimensional hidden layer. With data from the full temperature spectrum, Full_encoder encodes  spin distributions on a 12^2 grid to spin distributions on a 10^2 grid after 10,000 epochs. \n",
        "It takes about 70 minutes to train Full_encoder in 10,000 epochs with 6000 samples on a 20^2 grid (with GPU acceleration)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26whJDZn2VMn",
        "colab_type": "code",
        "outputId": "17e5a681-d874-44bb-b223-2ee31aecbb73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "'''\n",
        "Full_encoder is an RBM trained to explore how much features are preserved \n",
        "after Gibbs sampling. Train_data include T_L=0.27, T_C=2.27, T_H=4.27.\n",
        "'''\n",
        "\n",
        "# set RBM by entering dimensions of visible, hidder layers\n",
        "Full_encoder = RBMcdk(144,100) # full spectrum\n",
        "\n",
        "# put data in RBM with CDk method\n",
        "Full_encoder.contrastive_divergence(train_data,\n",
        "                                    iterations=10000,\n",
        "                                    k=2,\n",
        "                                    text_out=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples =  6000  with data dimension =  144\n",
            "training completed within  2353.962066888809\n",
            "Print model to 4535_weight/bias.csv for big data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf3xHXYRou0M",
        "colab_type": "text"
      },
      "source": [
        "## 3. Graph reconstructions\n",
        "The following code loads a pre-trained Full_encoder as loaded_RBM and plots four rows of eight graphs. The first row presents 8 samples from test_data. The second row presents the corresponding 8 reconstructed samples on the data-driven hidden layer ($h^{(0)}$). The third and fourth row present the corresponding 8 samples reconstructed after k=2000 Gibbs steps on the visible layer $v^{(k)}$ and the hidden layer $h^{(k)}$.\n",
        "By choosing the parameter *tempind*=(0,1,2) for $T=(T_L,T_C,T_H)$, the code plots data from every temp label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqWsf7I3SRHV",
        "colab_type": "code",
        "outputId": "11e152bb-a6fe-4c3a-a9cf-0698111a8348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "'''Load Trained Model from csv files'''\n",
        "loaded_RBM = RBMcdk(144,100) # need to know the dimensions of the loaded model\n",
        "\n",
        "# saved model's time stamp, found in message above\n",
        "#file_name = str(3223) # 3223 computes 20^2 grid for 2000 epochs. \n",
        "#file_name = str(1017) # 1017 computes 20^2 grid for 10000 epochs. \n",
        "file_name = str(4535) # 4535 computes 12^2 grid for 10000 epochs. \n",
        "path = '/content/gdrive/My Drive/IsingModel/'\n",
        "                       \n",
        "file_path = path+file_name+'_weight.csv'    \n",
        "loaded_RBM.weight = np.loadtxt(file_path,delimiter=\",\")\n",
        "\n",
        "file_path = path+file_name+'_vbias.csv'    \n",
        "loaded_RBM.visual_bias = np.loadtxt(file_path,delimiter=\",\")                       \n",
        "\n",
        "file_path = path+file_name+'_hbias.csv'    \n",
        "loaded_RBM.hidden_bias = np.loadtxt(file_path,delimiter=\",\")\n",
        "\n",
        "# plot 8 spin distributions from the validation dataset\n",
        "'''\n",
        "Full_encoder reserves visible features in hidden layer at Low T/ High T.\n",
        "'''\n",
        "tempind = 1  # pick a temp catagory, (0,1,2) = (T_L,T_C,T_H)\n",
        "validation_data = [0]*8\n",
        "np.random.seed(321)\n",
        "\n",
        "ind = 2000*tempind+np.random.randint(0,1999)\n",
        "print('Graphs below show %s distributions at %s%g.' % ('sample','T_L = ',temperatures[tempind]))\n",
        "plt.figure(figsize=(11,11))\n",
        "for i in range(8):\n",
        "    validation_data[i] = test_data[ind+i] #pick random 8 data\n",
        "    plt.subplot(1,8,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.reshape(validation_data[i],(12,12)), cmap=plt.cm.binary)\n",
        "    plt.xlabel(temperatures[test_labels[ind+i]])\n",
        "plt.show()\n",
        "\n",
        "# RECONSTRUCTION of validation test with trained RBM\n",
        "# plot 8 spin distributions from the reconstructed validation dataset\n",
        "\n",
        "\n",
        "# process validation data with trained RBM\n",
        "reconstructed_data = gibbs_sampling(validation_data, loaded_RBM, k=1, \n",
        "                                    return_state='H_data')\n",
        "reconstructed_label = [0]*8\n",
        "print('Graphs below show %s distributions.' % ('data-driven hidden',))\n",
        "plt.figure(figsize=(11,11))\n",
        "for i in range(8):\n",
        "    \n",
        "    # identify temperature with Temp Classifier\n",
        "    img = np.reshape(np.array(reconstructed_data[i]),(10,10))\n",
        "    img = (np.expand_dims(img,0)) # Add the image to a batch where it's the only member.\n",
        "    predictions = modeld10.predict(img)\n",
        "    reconstructed_label[i] = np.argmax(predictions)\n",
        "    \n",
        "    plt.subplot(1,8,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.reshape(reconstructed_data[i],(10,10)), cmap=plt.cm.binary)\n",
        "    plt.xlabel(classified_temp[reconstructed_label[i]])\n",
        "plt.show()\n",
        "\n",
        "# process validation data with trained RBM\n",
        "reconstructed_data = gibbs_sampling(validation_data, loaded_RBM, k=2000, \n",
        "                                    return_state='V_gibbs')\n",
        "reconstructed_label = [0]*8\n",
        "print('Graphs below show %s distributions.' % ('reconstructed visual',))\n",
        "plt.figure(figsize=(11,11))\n",
        "for i in range(8):\n",
        "    \n",
        "    # identify temperature with Temp Classifier\n",
        "    img = np.reshape(np.array(reconstructed_data[i]),(12,12))\n",
        "    img = (np.expand_dims(img,0)) # Add the image to a batch where it's the only member.\n",
        "    predictions = modeld12.predict(img)\n",
        "    reconstructed_label[i] = np.argmax(predictions)\n",
        "    \n",
        "    plt.subplot(1,8,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.reshape(reconstructed_data[i],(12,12)), cmap=plt.cm.binary)\n",
        "    plt.xlabel(classified_temp[reconstructed_label[i]])\n",
        "plt.show()\n",
        "\n",
        "# process validation data with trained RBM\n",
        "reconstructed_data = gibbs_sampling(validation_data, loaded_RBM, k=2000, \n",
        "                                    return_state='H_gibbs')\n",
        "reconstructed_label = [0]*8\n",
        "print('Graphs below show %s distributions.' % ('reconstructed hidden',))\n",
        "plt.figure(figsize=(11,11))\n",
        "for i in range(8):\n",
        "    \n",
        "    # identify temperature with Temp Classifier\n",
        "    img = np.reshape(np.array(reconstructed_data[i]),(10,10))\n",
        "    img = (np.expand_dims(img,0)) # Add the image to a batch where it's the only member.\n",
        "    predictions = modeld10.predict(img)\n",
        "    reconstructed_label[i] = np.argmax(predictions)\n",
        "    \n",
        "    plt.subplot(1,8,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.reshape(reconstructed_data[i],(10,10)), cmap=plt.cm.binary)\n",
        "    plt.xlabel(classified_temp[reconstructed_label[i]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graphs below show sample distributions at T_L = 2.27.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAABhCAYAAACu5mdDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHgBAYcoeAEBh/wPGevopNgRYPAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 792x792 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Graphs below show data-driven hidden distributions.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAABhCAYAAACu5mdDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wMW+xOQ6IW9DAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 792x792 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Graphs below show reconstructed visual distributions.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAABhCAYAAACu5mdDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 792x792 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Graphs below show reconstructed hidden distributions.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAABhCAYAAACu5mdDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntKQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 792x792 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxXO5RacP3i2",
        "colab_type": "text"
      },
      "source": [
        "One can still recognize the original input in the first row from the reconstructed visual distributions in the third row. But the second and the fourth rows show randomly irrelevant distributions and temp labels on the hidden layer. Apparently, the information stored on the hidden layer is not the compressed input even after the RBM learned the corresponding features. "
      ]
    }
  ]
}