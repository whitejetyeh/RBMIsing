{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Temp Classifier(June 19).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whitejetyeh/RBMIsing/blob/master/Temp_Classifier(June_19).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W5FSne7yXC5",
        "colab_type": "text"
      },
      "source": [
        "#Temperature Classifier\n",
        "With dataset from groups of spin distributions at different temperatures, this neural network, Temp_Classifier is trained to classify spin distributions into given temperature labels. Spin distributions are generated from Markov-Chain-Monte-Carlo Ising Model on on a $N_s$ by $N_s$ grid.\n",
        "\n",
        "\n",
        "Temp_Classifier can be pretty accurate 98+% with 1. enough neurons and layers, 2. finer grid, and 3. large gaps between temp labels. The last two points come from the nature of stochastic distributions, where we can spot identical spin distributions at different temperatures on a finite dimensional grid.\n",
        "\n",
        "\n",
        "2D Ising model has the critical temperature $T_C\\approx 2.27$. Below or above $T_C$, the system is in the ferromagnetic phase or the paramagnetic phase. The clear visual feature difference is the characteristic length, which refers to the size of a group of the same spin on the grid. At lower temperature, one spots larger groups of the same spin while up/down spins are much uniformly mixed up at higher temperature. Spin data in five temp_labels (0.27, 1.27, 2.27, 3.27, 4.27) are used here. \n",
        "\n",
        "Temp_Classifiers are named by $(N_s^2, N_{h}^{(1)}, N_{h}^{(2)},...,N_{l})$, where $(N_s, N_h^{(i)}, N_l)$ refer to (# of spins per dim, # of neurons on the i-th hidden layer, # of temp labels). One can easily build and test various temp_classifiers in different layers with **keras** in the following code. The accuracy of few classifiers are listed below.\n",
        "$$\\begin{align*}\n",
        "\\textrm{temp_classifier}&&\\textrm{test accuracy   }&&\\textrm{train accuracy}\\\\\n",
        "(20^2,512,1024,5) && 0.76 && 0.91\\\\\n",
        "(20^2,512,1024,3) && 0.98 && 1\\\\\n",
        "(10^2,256,512,3) && 0.97 && 0.99\\\\\n",
        "(10^2,256,512,5) && 0.64 && 0.81 \\end{align*}$$\n",
        "\n",
        "Temp_classifier $(10^2,256,512,5)$ is presented below, and we can see how overlapped distributions in different labels blur the classifier and limit the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz1fuerJy7qa",
        "colab_type": "text"
      },
      "source": [
        "## Acquire data\n",
        "\n",
        "\n",
        "1.   read data from google drive\n",
        "2.   reshape and label data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeGzfcfWyWHP",
        "colab_type": "code",
        "outputId": "e94ca38b-f8f5-4ddd-a092-bd549b0e1186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4JYRvITwFEz",
        "colab_type": "text"
      },
      "source": [
        "Import libraries needed for the computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpGr_NOifD6i",
        "colab_type": "code",
        "outputId": "060fd872-c7f6-4b2e-c656-4dc979cbc6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os.path\n",
        "\n",
        "print(tf.__version__)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdluHFrMLsGf",
        "colab_type": "text"
      },
      "source": [
        "The following code loads and processes raw data of spin distributions, which come from \n",
        "[MCMC Ising Model](https://drive.google.com/open?id=1GTzxGebfb-vao0J4q_CzSwC3SPmWakAd).\n",
        "\n",
        "Raw data in each csv file labeled by temperature save 2000 samples of spin distribution on a $N_s$ by $N_s$ grid in the shape of $(2000*N_s,N_s)$, and they need to be reshaped into $(2000,N_s,N_s)$ for **Keras** to read.\n",
        "\n",
        "Train_data and test_data are two independent datasets consisted of stacked up raw data. Train_data is randomly shuffled for training the model so the gradient descent method will less likely be stuck at the local minima, and test_data is for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_28JjQuqzXbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# given temperature labels from data set\n",
        "temperatures=[0.27, 1.27, 2.27, 3.27, 4.27]\n",
        "#temperatures=[0.27, 2.27, 4.27]\n",
        "\n",
        "# Load spin data from CSV file\n",
        "def acquire_spin_data(file, dataset):    \n",
        "    raw_data = np.loadtxt(file,delimiter=\",\")\n",
        "    raw_data = (raw_data+1)/2 # scale data from spin=(1,-1) to (1,0)\n",
        "    raw_data=raw_data.reshape((2000,10,10)) # reshape to 2k data of 10^2 grid\n",
        "    #raw_data=raw_data.reshape((2000,20,20)) # reshape to 2k data of 20^2 grid\n",
        "    # tensor_util.py in tf takes only plain python lists or tuples but not numpy arrays\n",
        "    raw_data=raw_data.astype(np.float32).tolist() \n",
        "    dataset.extend(raw_data)\n",
        "\n",
        "train_data, test_data = [], []\n",
        "train_labels, test_labels = [], []\n",
        "ind = 0 # counter for the labels\n",
        "for temp in temperatures:\n",
        "    train_file = '/content/gdrive/My Drive/IsingModel/training2k/t='+str(temp)+'.csv'\n",
        "    test_file = '/content/gdrive/My Drive/IsingModel/validation2k/t='+str(temp)+'.csv'\n",
        "    acquire_spin_data(train_file, train_data)\n",
        "    acquire_spin_data(test_file, test_data)\n",
        "    train_labels += [ind]*2000\n",
        "    test_labels += [ind]*2000\n",
        "    ind += 1\n",
        "\n",
        "# shuffle dataset and labels simutaneously\n",
        "# array indexing\n",
        "def unity_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "train_data,train_labels=unity_shuffled_copies(np.array(train_data),np.array(train_labels))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvcdcce9gd7N",
        "colab_type": "text"
      },
      "source": [
        "## Explore data\n",
        "Take a look at spin distributions from the training dataset.\n",
        "16 spin distributions with their temperatures denoted below are presented.\n",
        "\n",
        "It's clear to see the larger characteristic length at lower temperature, i.e. larger block of same spin in the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8q57PZjQSxW",
        "colab_type": "code",
        "outputId": "19402666-81c0-4344-ba7e-31a6dd63d840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "# plot 16 spin distributions from the training dataset\n",
        "np.random.seed(1)\n",
        "ind=np.random.randint(0,1999) #pick random 16 data\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(train_data[ind+i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(temperatures[train_labels[ind+i]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAI/CAYAAACf7mYiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emIgKwhdbfjn",
        "colab_type": "text"
      },
      "source": [
        "## Overlapped distribution figures\n",
        "\n",
        "\n",
        "Notice that we might find the same spin distribution in two temperature classes. \n",
        "In particular, it's easy to find the all up/down (all black or white) distribution can appear at both t=0.27 and t=1.27. The overlapped distributions limit the accuracy of the classifier on a finite grid, and therefore the temperature difference between labels can't be too small. In this case of 10 by 10 grid, we can only specify $t=\\{0.27,1.27,2.27,3.27,4.27\\}$ with accuracy 64%, but the accuracy becomes 97% with less labels $t=\\{0.27,2.27,4.27\\}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izzG2KuPeEU9",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network\n",
        "\n",
        "\n",
        "1.   visible layer: 100 neurons for spins on the 10 by 10 grid (upgrated to 20 x 20)\n",
        "2.   hidden layer: 64 neurons with activation function = tanh\n",
        "3.   output layer: 5 neurons for 5 temperatures with activation function = softmax\n",
        "4.   cost function: cross entropy\n",
        "5.   gradient descent: learning rate=0.1 for 7000 epochs\n",
        "\n",
        "\n",
        "Options for the optimizer could be adam, sgd, RMSprop,...etc; computation time and accuracy are not significantly different. Wtih 70 epochs, the accuracy in the training data can achieve near 90%, and the accuracy in the validation data is about 60~70%. \n",
        "Here, adam converges faster, but sgd achieves higher accuracy. Since the number of required epochs is quite small, we prefer sgd over adam.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O21APqgSQbMl",
        "colab_type": "text"
      },
      "source": [
        "## Model Construction\n",
        "This example has four layers. First input layer takes flatten spin distributions on a 10 by 10 grid. The next two layers have 256 and 512 neurons with activation function **relu**. One can change relu to tanh or sigmoid, and the difference is insignificant. The last layer with **softmax** outputs the probabilities of corresponding temp labels. Finally, the loss function is **sgd**. In this case, sgd works better than the usual adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DltrqXnof_U0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up the layers\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(10, 10)),\n",
        "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(5, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='sgd', \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEfFyTl7QXfU",
        "colab_type": "text"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "The training doesn't take longer than a minute. By looking at accuracy of each epoch, 70 epochs is enough to reach a steady result with a small fluctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUsdsQOW2OcM",
        "colab_type": "code",
        "outputId": "5aa02443-d2ad-485d-eefd-144d3fa870d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2534
        }
      },
      "source": [
        "# train the model in numpy arrays\n",
        "start = time.time() # timing training process\n",
        "model.fit(train_data,train_labels, epochs=70)\n",
        "end = time.time() # timing training process\n",
        "print('training time takes ',end-start)\n",
        "\n",
        "# evaluate accuracy\n",
        "test_loss, test_acc = model.evaluate(np.array(test_data),test_labels)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "10000/10000 [==============================] - 1s 63us/sample - loss: 1.4650 - acc: 0.3665\n",
            "Epoch 2/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 1.3178 - acc: 0.4088\n",
            "Epoch 3/70\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 1.2328 - acc: 0.4405\n",
            "Epoch 4/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 1.1572 - acc: 0.4732\n",
            "Epoch 5/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 1.0878 - acc: 0.5237\n",
            "Epoch 6/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 1.0205 - acc: 0.5674\n",
            "Epoch 7/70\n",
            "10000/10000 [==============================] - 1s 63us/sample - loss: 0.9579 - acc: 0.5984\n",
            "Epoch 8/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.9021 - acc: 0.6158\n",
            "Epoch 9/70\n",
            "10000/10000 [==============================] - 1s 62us/sample - loss: 0.8533 - acc: 0.6312\n",
            "Epoch 10/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.8107 - acc: 0.6373\n",
            "Epoch 11/70\n",
            "10000/10000 [==============================] - 1s 62us/sample - loss: 0.7746 - acc: 0.6468\n",
            "Epoch 12/70\n",
            "10000/10000 [==============================] - 1s 63us/sample - loss: 0.7462 - acc: 0.6523\n",
            "Epoch 13/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.7240 - acc: 0.6556\n",
            "Epoch 14/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.7049 - acc: 0.6609\n",
            "Epoch 15/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.6873 - acc: 0.6663\n",
            "Epoch 16/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.6742 - acc: 0.6714\n",
            "Epoch 17/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.6621 - acc: 0.6769\n",
            "Epoch 18/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.6509 - acc: 0.6720\n",
            "Epoch 19/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.6423 - acc: 0.6751\n",
            "Epoch 20/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.6323 - acc: 0.6791\n",
            "Epoch 21/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.6233 - acc: 0.6869\n",
            "Epoch 22/70\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.6157 - acc: 0.6896\n",
            "Epoch 23/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.6087 - acc: 0.6948\n",
            "Epoch 24/70\n",
            "10000/10000 [==============================] - 1s 62us/sample - loss: 0.6002 - acc: 0.7019\n",
            "Epoch 25/70\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.5927 - acc: 0.6993\n",
            "Epoch 26/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.5875 - acc: 0.7023\n",
            "Epoch 27/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.5808 - acc: 0.7104\n",
            "Epoch 28/70\n",
            "10000/10000 [==============================] - 1s 62us/sample - loss: 0.5759 - acc: 0.7081\n",
            "Epoch 29/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.5692 - acc: 0.7095\n",
            "Epoch 30/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.5633 - acc: 0.7155\n",
            "Epoch 31/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.5586 - acc: 0.7158\n",
            "Epoch 32/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.5566 - acc: 0.7175\n",
            "Epoch 33/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.5470 - acc: 0.7213\n",
            "Epoch 34/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.5401 - acc: 0.7252\n",
            "Epoch 35/70\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.5358 - acc: 0.7298\n",
            "Epoch 36/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.5297 - acc: 0.7308\n",
            "Epoch 37/70\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.5257 - acc: 0.7345\n",
            "Epoch 38/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.5187 - acc: 0.7418\n",
            "Epoch 39/70\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.5156 - acc: 0.7368\n",
            "Epoch 40/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.5083 - acc: 0.7456\n",
            "Epoch 41/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.5048 - acc: 0.7453\n",
            "Epoch 42/70\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.5007 - acc: 0.7499\n",
            "Epoch 43/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.4929 - acc: 0.7530\n",
            "Epoch 44/70\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.4894 - acc: 0.7515\n",
            "Epoch 45/70\n",
            "10000/10000 [==============================] - 1s 62us/sample - loss: 0.4843 - acc: 0.7599\n",
            "Epoch 46/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.4794 - acc: 0.7585\n",
            "Epoch 47/70\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.4729 - acc: 0.7659\n",
            "Epoch 48/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.4682 - acc: 0.7697\n",
            "Epoch 49/70\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.4638 - acc: 0.7688\n",
            "Epoch 50/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.4596 - acc: 0.7660\n",
            "Epoch 51/70\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.4534 - acc: 0.7735\n",
            "Epoch 52/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.4497 - acc: 0.7775\n",
            "Epoch 53/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.4419 - acc: 0.7799\n",
            "Epoch 54/70\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.4390 - acc: 0.7794\n",
            "Epoch 55/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.4313 - acc: 0.7822\n",
            "Epoch 56/70\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.4279 - acc: 0.7876\n",
            "Epoch 57/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.4278 - acc: 0.7861\n",
            "Epoch 58/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.4185 - acc: 0.7919\n",
            "Epoch 59/70\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.4140 - acc: 0.7945\n",
            "Epoch 60/70\n",
            "10000/10000 [==============================] - 1s 62us/sample - loss: 0.4097 - acc: 0.7972\n",
            "Epoch 61/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.4060 - acc: 0.7991\n",
            "Epoch 62/70\n",
            "10000/10000 [==============================] - 1s 62us/sample - loss: 0.3999 - acc: 0.8036\n",
            "Epoch 63/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.3981 - acc: 0.8039\n",
            "Epoch 64/70\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.3954 - acc: 0.7993\n",
            "Epoch 65/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.3881 - acc: 0.8075\n",
            "Epoch 66/70\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.3806 - acc: 0.8116\n",
            "Epoch 67/70\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.3787 - acc: 0.8112\n",
            "Epoch 68/70\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.3744 - acc: 0.8142\n",
            "Epoch 69/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.3727 - acc: 0.8115\n",
            "Epoch 70/70\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 0.3658 - acc: 0.8175\n",
            "training time takes  42.620079040527344\n",
            "10000/10000 [==============================] - 0s 43us/sample - loss: 0.7780 - acc: 0.6421\n",
            "Test accuracy: 0.6421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wILapJFSQ_t3",
        "colab_type": "text"
      },
      "source": [
        "Save the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vvYv6iI2b2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model and architecture to single file\n",
        "save_path = '/content/gdrive/My Drive/IsingModel/'\n",
        "model_name = 'temp_classifier_100_256_512_5.h5'\n",
        "model.save(os.path.join(save_path, model_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVdob9MMt6h2",
        "colab_type": "text"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEmvBT6qVkkZ",
        "colab_type": "text"
      },
      "source": [
        "One can load a trained temp_classifier as the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo_PXkCgVidf",
        "colab_type": "code",
        "outputId": "936b6649-3843-4eaf-984f-f166a5164c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "# load model and architecture from a single file\n",
        "save_path = '/content/gdrive/My Drive/IsingModel/'\n",
        "model_name = 'temp_classifier_100_256_512_5.h5'\n",
        "model = keras.models.load_model(os.path.join(save_path, model_name))\n",
        "\n",
        "model.summary() # print the architecture of the model\n",
        "\n",
        "# evaluate accuracy\n",
        "test_loss, test_acc = model.evaluate(np.array(test_data),test_labels)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 160,005\n",
            "Trainable params: 160,005\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.7780 - acc: 0.6421\n",
            "Test accuracy: 0.6421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_fBvtXfVzuv",
        "colab_type": "text"
      },
      "source": [
        "Given data of spin distributions, temp_classifier can predict the corresponding temperatures as the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoZVc_YBp8_E",
        "colab_type": "code",
        "outputId": "5ee1fc69-b9ab-41d8-e862-b6023bb75cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "# make a prediction\n",
        "TempClass = 2 # choose a temp label\n",
        "chosen_index = TempClass*2000+np.random.randint(0,1999) # randomly pick a sample\n",
        "\n",
        "img = np.array(test_data[chosen_index])\n",
        "img = (np.expand_dims(img,0)) # Add the image to a batch where it's the only member.\n",
        "predictions = model.predict(img)\n",
        "\n",
        "#plot_value_array(0, predictions, test_labels[chosen_index])\n",
        "#_ = plt.xticks(range(5), temperatures, rotation=45)\n",
        "\n",
        "print(predictions) # probability distribution\n",
        "plt.plot(range(len(temperatures)),predictions[0],'ob')\n",
        "plt.show()\n",
        "print('Predict temperature is '+\n",
        "      str(temperatures[np.argmax(predictions)])) # prediction\n",
        "\n",
        "print('Data temperature is '+str(temperatures[test_labels[chosen_index]]))# fact\n",
        "plt.figure()\n",
        "plt.imshow(test_data[chosen_index], cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.7254787e-08 8.9285886e-06 4.6767202e-01 5.0977039e-01 2.2548644e-02]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADylJREFUeJzt3W2MXOddhvHrjh0DphWV6pWIYicb\nwF9MKW26mKBKpSqt5FBkI7VIjrbQoFYWL1aLggSBoCCC8qGtVBBgqV21kQq4uCEgtC2uoooWIT40\neNOmL04ILFGcOKqU7QspyNBg8ufDjJvtMus9s56dmX18/aTVzjnn8Z5bT3Junz0zxydVhSSpLddM\nOoAkafQsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDdk5qx3v27KnZ2dlJ7V6S\ntqWHH374q1U1s9G4iZX77OwsS0tLk9q9JG1LSc51GedlGUlqkOUuSQ2y3CWpQZa7JDXIcpekBlnu\nkjbt5EmYnYVrrul9P3ly0ol0ycQ+Cilpezt5Eo4dgwsXesvnzvWWAebnJ5dLPZ65S9qUu+56sdgv\nuXCht16T16nckxxK8niS5SR3Dth+e5KVJI/0v945+qiSpslTTw23XuO14WWZJDuAE8CbgPPAmSSL\nVfXomqEfq6rjW5BR0hS64YbepZhB6zV5Xc7cDwLLVfVEVT0PnAKObG0sSdPu3nth9+7vXLd7d2+9\nJq9LuV8PPL1q+Xx/3VpvSfLFJA8k2TeSdJKm1vw8LCzAjTdC0vu+sOCbqdNiVG+ofhyYrapXAp8C\nPjJoUJJjSZaSLK2srIxo15ImZX4ennwSXnih991inx5dyv0ZYPWZ+N7+um+rqq9V1bf6ix8CXjPo\nB1XVQlXNVdXczMyG/2KlJGmTupT7GWB/kpuS7AKOAourByS5btXiYeCx0UWUJA1rw0/LVNXFJMeB\nB4EdwH1VdTbJPcBSVS0C70pyGLgIfB24fQszS5I2kKqayI7n5ubKh3VI0nCSPFxVcxuN8w5VSWqQ\n5S5JDbLcJalBlrskNchyl6QGWe5Snw+eUEt8WIeED55Qezxzl/DBE2qP5S7hgyfUHstdYv0HTPjg\nCW1XlruED55Qeyx3CR88ofb4aRmpb37eMlc7PHOXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLc\nJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBnUq9ySHkjye\nZDnJnZcZ95YklWRudBElScPasNyT7ABOALcCB4DbkhwYMO6lwLuBh0YdUpI0nC5n7geB5ap6oqqe\nB04BRwaM+33gPcB/jzCfJGkTupT79cDTq5bP99d9W5KbgX1V9bcjzCZJ2qQrfkM1yTXA+4Ff7zD2\nWJKlJEsrKytXumtJ0jq6lPszwL5Vy3v76y55KfAK4O+TPAncAiwOelO1qhaqaq6q5mZmZjafWpJ0\nWV3K/QywP8lNSXYBR4HFSxur6rmq2lNVs1U1C3wWOFxVS1uSWJK0oQ3LvaouAseBB4HHgPur6myS\ne5Ic3uqAkqTh7ewyqKpOA6fXrLt7nbGvv/JYkqQr4R2qktQgy12SGmS5S1KDLHdJapDlLkkNstwl\nqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIa\nZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGd\nyj3JoSSPJ1lOcueA7b+U5EtJHknyj0kOjD6qJKmrDcs9yQ7gBHArcAC4bUB5f7SqfqSqXgW8F3j/\nyJNKkjrrcuZ+EFiuqieq6nngFHBk9YCq+uaqxe8FanQRJUnD2tlhzPXA06uWzwM/vnZQkl8F7gB2\nAW8YSTpJ0qaM7A3VqjpRVT8I/CbwO4PGJDmWZCnJ0srKyqh2LUlao0u5PwPsW7W8t79uPaeAnx20\noaoWqmququZmZma6p5QkDaVLuZ8B9ie5Kcku4CiwuHpAkv2rFt8M/OvoIkqShrXhNfequpjkOPAg\nsAO4r6rOJrkHWKqqReB4kjcC/wN8A3j7VoaWJF1elzdUqarTwOk16+5e9frdI84lSboC3qEqSQ2y\n3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtd\nkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWp\nQZa7JDXIcpekBlnuktQgy12SGtSp3JMcSvJ4kuUkdw7YfkeSR5N8McnfJblx9FElSV1tWO5JdgAn\ngFuBA8BtSQ6sGfZ5YK6qXgk8ALx31EElSd11OXM/CCxX1RNV9TxwCjiyekBVfaaqLvQXPwvsHW1M\nSdIwupT79cDTq5bP99et5x3AJwdtSHIsyVKSpZWVle4pJUlDGekbqkneBswB7xu0vaoWqmququZm\nZmZGuWtJ0io7O4x5Bti3anlvf913SPJG4C7gJ6vqW6OJJ0najC5n7meA/UluSrILOAosrh6Q5NXA\nB4HDVfXs6GNKkoaxYblX1UXgOPAg8Bhwf1WdTXJPksP9Ye8DXgL8ZZJHkiyu8+MkSWPQ5bIMVXUa\nOL1m3d2rXr9xxLkkSVfAO1QlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalB\nlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5\nS1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhrUqdyTHEryeJLlJHcO2P66JJ9LcjHJ\nW0cfU5I0jA3LPckO4ARwK3AAuC3JgTXDngJuBz466oCSpOHt7DDmILBcVU8AJDkFHAEevTSgqp7s\nb3thCzJKkobU5bLM9cDTq5bP99dJkqbUWN9QTXIsyVKSpZWVlXHuWpKuKl3K/Rlg36rlvf11Q6uq\nhaqaq6q5mZmZzfwISVIHXcr9DLA/yU1JdgFHgcWtjSVJuhIblntVXQSOAw8CjwH3V9XZJPckOQyQ\n5MeSnAd+DvhgkrNbGVqSdHldPi1DVZ0GTq9Zd/eq12foXa6RJE0B71CVpAZZ7pLUIMtdkhpkuUtS\ngyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXI\ncpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlaUxOnoTZWbjmmt73kye3bl87t+5HS5IuOXkS\njh2DCxd6y+fO9ZYB5udHvz/P3CVpDO6668Viv+TChd76rWC5S9IYPPXUcOuvlOUuSWNwww3Drb9S\nlrskjcG998Lu3d+5bvfu3vqtYLlL0hjMz8PCAtx4IyS97wsLW/NmKvhpGUkam/n5rSvztTqduSc5\nlOTxJMtJ7hyw/buSfKy//aEks6MOKknqbsNyT7IDOAHcChwAbktyYM2wdwDfqKofAv4AeM+og8J4\nbwBogfMlXb26nLkfBJar6omqeh44BRxZM+YI8JH+6weAn0qS0cV88QaAc+eg6sUbACyswZwv6erW\npdyvB55etXy+v27gmKq6CDwHvHwUAS8Z9w0A253zJV3dxvppmSTHkiwlWVpZWRnqz477BoDtzvmS\nrm5dyv0ZYN+q5b39dQPHJNkJfB/wtbU/qKoWqmququZmZmaGCjruGwC2O+dLurp1KfczwP4kNyXZ\nBRwFFteMWQTe3n/9VuDTVVWjizn+GwC2O+dLurptWO79a+jHgQeBx4D7q+psknuSHO4P+zDw8iTL\nwB3A//u45JUa9w0A253zJV3dMuIT7M7m5uZqaWlpIvuWpO0qycNVNbfROP/5AUlqkOUuSQ2y3CWp\nQZa7JDXIcpekBk3s0zJJVoBzm/zje4CvjjDOqJhrOOYa3rRmM9dwriTXjVW14V2gEyv3K5FkqctH\ngcbNXMMx1/CmNZu5hjOOXF6WkaQGWe6S1KDtWu4Lkw6wDnMNx1zDm9Zs5hrOlufaltfcJUmXt13P\n3CVJlzHV5T6tD+bukOv2JCtJHul/vXNMue5L8mySL6+zPUn+qJ/7i0lunpJcr0/y3Kr5unsMmfYl\n+UySR5OcTfLuAWPGPl8dc01ivr47yT8l+UI/1+8NGDP247Fjrokcj/1970jy+SSfGLBta+erqqby\nC9gB/BvwA8Au4AvAgTVjfgX4QP/1UeBjU5LrduBPJjBnrwNuBr68zvafBj4JBLgFeGhKcr0e+MSY\n5+o64Ob+65cC/zLgv+PY56tjrknMV4CX9F9fCzwE3LJmzCSOxy65JnI89vd9B/DRQf+9tnq+pvnM\nfSoezL3JXBNRVf8AfP0yQ44Af1o9nwVeluS6Kcg1dlX1lar6XP/1f9B7VsHaZwOPfb465hq7/hz8\nZ3/x2v7X2jfsxn48dsw1EUn2Am8GPrTOkC2dr2ku96l4MPcmcwG8pf+r/ANJ9g3YPglds0/CT/R/\ntf5kkh8e5477vw6/mt5Z32oTna/L5IIJzFf/EsMjwLPAp6pq3fka4/HYJRdM5nj8Q+A3gBfW2b6l\n8zXN5b6dfRyYrapXAp/ixb+dNdjn6N1S/aPAHwN/M64dJ3kJ8FfAr1XVN8e1341skGsi81VV/1tV\nr6L3HOWDSV4xjv1upEOusR+PSX4GeLaqHt7qfa1nmst9ZA/mHneuqvpaVX2rv/gh4DVbnKmrLnM6\ndlX1zUu/WlfVaeDaJHu2er9JrqVXoCer6q8HDJnIfG2Ua1LztWr//w58Bji0ZtMkjscNc03oeHwt\ncDjJk/Qu3b4hyZ+vGbOl8zXN5T4VD+beTK4112UP07tuOg0WgV/ofwrkFuC5qvrKpEMl+f5L1xqT\nHKT3/+WWlkJ/fx8GHquq968zbOzz1SXXhOZrJsnL+q+/B3gT8M9rho39eOySaxLHY1X9VlXtrapZ\neh3x6ap625phWzpfO0f1g0atqi4mufRg7h3AfdV/MDewVFWL9A6CP0vvwdxfpzeJ05DrXek9PPxi\nP9ftW50LIMlf0PskxZ4k54HfpfcGE1X1AeA0vU+ALAMXgF+cklxvBX45yUXgv4CjY/hL+rXAzwNf\n6l+vBfht4IZVuSYxX11yTWK+rgM+kmQHvb9M7q+qT0z6eOyYayLH4yDjnC/vUJWkBk3zZRlJ0iZZ\n7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNej/ABBkkOVEO0N1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Predict temperature is 3.27\n",
            "Data temperature is 2.27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAACbFJREFUeJzt3c/rZXUdx/HnqxnFZowSclEzQ85C\nCglC/SKWIdEUFEq2aKFgkJvZ9GOMQsy/QcQWIQxWBEouJhchkS5y0Wrw6xjYzGiImY4aTUQpbUbx\n3eJ7g1Gae8/M957OvW+fj9V875zvnTfX+/Rzz7n3nJuqQlJPH5h6AEnjMXCpMQOXGjNwqTEDlxoz\ncKkxA5caM3CpMQOXGts5xp0m8eNx0siqKou2cQWXGjNwqTEDlxozcKkxA5caM3CpMQOXGhsUeJKv\nJHk+yQtJ7h57KEnLkUWXbEqyA/gT8GXgFPAUcFtVnZjzO37QRRrZsj7och3wQlW9WFVngEeAW7Y7\nnKTxDQl8D/DKWT+fmt32LkkOJtlMsrms4SRtz9I+i15Vh4HD4Et0aVUMWcFfBfad9fPe2W2SVtyQ\nwJ8CrkyyP8nFwK3Ar8cdS9IyLHyJXlVvJ/kO8DiwA/hZVR0ffTJJ27bwbbILulP3waXReT649D5n\n4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41ZuBSYwYuNWbg\nUmMGLjVm4FJjBi41ZuBSYwYuNWbgUmMGLjVm4FJjBi41trTvB9e7jfGdb2NKFn7NldaQK7jUmIFL\njRm41JiBS40ZuNSYgUuNLQw8yb4kTyY5keR4kkP/j8EkbV8WvV+b5GPAx6rqWJIPAU8DX6+qE3N+\nZ73eBB6B74NrbFW18D/awhW8ql6vqmOzP78JnAT2bH88SWM7r33wJFcAVwNHxxhG0nIN/qhqkkuB\nXwF3VtUb/+PvDwIHlzibpG1auA8OkOQi4DHg8aq6b8D267UDOgL3wTW2IfvgQw6yBfgF8I+qunPI\nP2zgBq7xLSvwzwO/B54F3pndfE9V/WbO76zXs3sEBq6xLSXwC2HgBq7xLeVtMknry8ClxgxcaszA\npcYMXGrMiy6OxKPS62esdz6mfC64gkuNGbjUmIFLjRm41JiBS40ZuNSYgUuNGbjUmIFLjRm41JiB\nS40ZuNSYgUuNGbjUmIFLjRm41JiBS40ZuNSYgUuNGbjUmIFLjXlV1ZF0vEJndx0fW1dwqTEDlxoz\ncKkxA5caM3CpMQOXGjNwqbHBgSfZkeSZJI+NOZCk5TmfFfwQcHKsQSQt36DAk+wFbgIeHHccScs0\ndAW/H7gLeOdcGyQ5mGQzyeZSJpO0bQsDT3Iz8LeqenredlV1uKo2qmpjadNJ2pYhK/gNwNeSvAQ8\nAnwxyUOjTiVpKXI+Zz0l+QLww6q6ecF245xKtUY8m0xjq6qFTwbfB5caO68VfPCduoK7gmt0ruDS\n+5yBS40ZuNSYgUuNGbjUmFdVHYlHu7UKXMGlxgxcaszApcYMXGrMwKXGDFxqzMClxgxcaszApcYM\nXGrMwKXGDFxqzMClxgxcaszApcYMXGrMwKXGDFxqzMClxgxcaszApcZGuarqtddey+bm5hh3vTa8\nquoWv6NtnMdgY2Nj0Hau4FJjBi41ZuBSYwYuNWbgUmMGLjU2KPAkH0lyJMlzSU4m+ezYg0navqHv\ng/8Y+G1VfSPJxcCuEWeStCQLA0/yYeBG4FsAVXUGODPuWJKWYchL9P3AaeDnSZ5J8mCS3SPPJWkJ\nhgS+E7gGeKCqrgb+Ddz93o2SHEyymWTz9OnTSx5T0oUYEvgp4FRVHZ39fISt4N+lqg5X1UZVbVx+\n+eXLnFHSBVoYeFX9FXglySdnNx0ATow6laSlGHoU/bvAw7Mj6C8Cd4w3kqRlGRR4Vf0BGHZ+mqSV\n4SfZpMYMXGrMwKXGDFxqzMClxgxcamyUq6rKq4mObazHtxtXcKkxA5caM3CpMQOXGjNwqTEDlxoz\ncKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5cayxgXr0syyhXx\nvNCeF10c07pdKLOqFt6xK7jUmIFLjRm41JiBS40ZuNSYgUuNGbjU2KDAk3w/yfEkf0zyyySXjD2Y\npO1bGHiSPcD3gI2q+jSwA7h17MEkbd/Ql+g7gQ8m2QnsAl4bbyRJy7Iw8Kp6FbgXeBl4HfhXVT3x\n3u2SHEyymWRz+WNKuhBDXqJfBtwC7Ac+DuxOcvt7t6uqw1W1UVUbyx9T0oUY8hL9S8Cfq+p0Vb0F\nPAp8btyxJC3DkMBfBq5Psitbp8UcAE6OO5akZRiyD34UOAIcA56d/c7hkeeStASeD75mPB98PJ4P\nLmmtGLjUmIFLjRm41JiBS43tnHoArYZ1O4I8hnWadShXcKkxA5caM3CpMQOXGjNwqTEDlxozcKkx\nA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpMQOXGjNwqTEDlxozcKkxA5caM3CpsbGuqvp34C8D\ntvvobNtBVuCql+c178R8bMezCrN+YshGo3z54FBJNqtqY7IBztM6zbtOs8J6zbtOs/oSXWrMwKXG\npg788MT//vlap3nXaVZYr3nXZtZJ98EljWvqFVzSiCYLPMlXkjyf5IUkd081xyJJ9iV5MsmJJMeT\nHJp6piGS7EjyTJLHpp5lniQfSXIkyXNJTib57NQzzZPk+7PnwR+T/DLJJVPPNM8kgSfZAfwE+Cpw\nFXBbkqummGWAt4EfVNVVwPXAt1d41rMdAk5OPcQAPwZ+W1WfAj7DCs+cZA/wPWCjqj4N7ABunXaq\n+aZawa8DXqiqF6vqDPAIcMtEs8xVVa9X1bHZn99k6wm4Z9qp5kuyF7gJeHDqWeZJ8mHgRuCnAFV1\npqr+Oe1UC+0EPphkJ7ALeG3ieeaaKvA9wCtn/XyKFY8GIMkVwNXA0WknWeh+4C7gnakHWWA/cBr4\n+Wx34sEku6ce6lyq6lXgXuBl4HXgX1X1xLRTzedBtoGSXAr8Crizqt6Yep5zSXIz8LeqenrqWQbY\nCVwDPFBVVwP/Blb5eMxlbL3S3A98HNid5PZpp5pvqsBfBfad9fPe2W0rKclFbMX9cFU9OvU8C9wA\nfC3JS2zt+nwxyUPTjnROp4BTVfXfV0RH2Ap+VX0J+HNVna6qt4BHgc9NPNNcUwX+FHBlkv1JLmbr\nQMWvJ5plrmydhfFT4GRV3Tf1PItU1Y+qam9VXcHW4/q7qlrJVaaq/gq8kuSTs5sOACcmHGmRl4Hr\nk+yaPS8OsMIHBWG8s8nmqqq3k3wHeJytI5E/q6rjU8wywA3AN4Fnk/xhdts9VfWbCWfq5LvAw7P/\n0b8I3DHxPOdUVUeTHAGOsfXuyjOs+Kfa/CSb1JgH2aTGDFxqzMClxgxcaszApcYMXGrMwKXGDFxq\n7D8tn1IxS+LJNgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}